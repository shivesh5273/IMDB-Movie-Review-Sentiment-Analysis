{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Applying Machine Learning to Sentiment Analysis\n",
    "## Project: IMDB Movie Review Data\n",
    "In the mordern internet and social media age, people's opinion, reviews and recommendations have become a valuable resource for businesses.\n",
    "Thanks to modern technology, we are now able to collect and analyse such data more efficiently.\n",
    "In this project, I will delve into the subfield of **Natural Language Processing** called **Sentiment Analysis**, and learn how to use machine learning algorithms to classify documents based on their popularity.\n",
    " **Sentiment Analysis** is also called as **Opinion Mining**."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Applying Machine Learning to Sentiment Analysis\n",
    "## Obtaining the Movie Review Dataset\n",
    "\n",
    "In this section, I prepare the **IMDB movie review dataset** that will be used\n",
    "throughout the sentiment analysis project.\n",
    "\n",
    "The goal is to make sure that:\n",
    "\n",
    "1. The notebook is running inside the correct project folder.\n",
    "2. A `data/aclImdb` directory exists and contains the extracted IMDB dataset.\n",
    "3. If the dataset is missing, the notebook prints clear instructions on what the\n",
    "   expected folder structure should look like."
   ],
   "id": "e080a0c270afb82b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step A – Confirm the notebook location\n",
    "\n",
    "Before touching any data, I verify that the notebook is running inside the\n",
    "expected project folder.\n",
    "\n",
    "This small check helps avoid subtle bugs later, for example:\n",
    "- running the notebook from a different directory,\n",
    "- saving files to the wrong place,\n",
    "- or accidentally creating duplicate `data/` folders.\n",
    "\n",
    "The next cell prints the **current working directory** so I can visually confirm\n",
    "that it matches:\n",
    "\n",
    "`/Users/shivesh/Desktop/PythonProject/Sentiment Analysis`"
   ],
   "id": "58970f1ca74454b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:40:27.287867Z",
     "start_time": "2025-12-18T16:40:27.284050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Double Check where the notebook is running\n",
    "import os\n",
    "from cProfile import label\n",
    "\n",
    "os.getcwd()"
   ],
   "id": "2e22a1fd4f5f549d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shivesh/Desktop/PythonProject/Sentiment Analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step B – Make sure the IMDB dataset is available in `data/aclImdb`\n",
    "\n",
    "The IMDB movie review dataset is distributed as the archive\n",
    "`aclImdb_v1.tar` (originally `aclImdb_v1.tar.gz`).\n",
    "\n",
    "On this machine I downloaded and extracted it **manually**:\n",
    "\n",
    "1. Downloaded the archive from the Stanford URL in Safari.\n",
    "2. Moved the file into the `Sentiment Analysis` project folder.\n",
    "3. Extracted it (by double-clicking in Finder), which created a folder:\n",
    "\n",
    "   `aclImdb/`  containing `train/` and `test/` subfolders.\n",
    "\n",
    "After that, I want the project to follow this structure:\n",
    "\n",
    "```text\n",
    "Sentiment Analysis/\n",
    "  sample.ipynb\n",
    "  data/\n",
    "    aclImdb/\n",
    "      train/\n",
    "      test/\n",
    "      ..."
   ],
   "id": "1018c02e1b9267bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:40:42.105614Z",
     "start_time": "2025-12-18T16:40:42.102161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Folder inside the project where we keep raw data\n",
    "DATA_DIR = \"data\"\n",
    "IMDB_DIR = os.path.join(DATA_DIR, \"aclImdb\")\n",
    "\n",
    "# Make sure the data folder exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Check that the extracted dataset is in the right place\n",
    "if os.path.isdir(IMDB_DIR):\n",
    "    print(\"IMDB dataset is ready at:\", IMDB_DIR)\n",
    "else:\n",
    "    print(\"IMDB dataset NOT found at:\", IMDB_DIR)\n",
    "    print()\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"  Sentiment Analysis/\")\n",
    "    print(\"    sample.ipynb\")\n",
    "    print(\"    data/aclImdb/{train,test,...}\")\n",
    "    raise FileNotFoundError(f\"Expected folder not found: {IMDB_DIR}\")"
   ],
   "id": "3a42959e0f604f61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB dataset is ready at: data/aclImdb\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the movie dataset into a more convinient format\n",
    "To visualize the progress and estimated time until completion, we will use the **Python Progress Indicator**.\n",
    "PyPind can be installed by executing the _\"pip install pyprind\"_ command in the terminal."
   ],
   "id": "1b415af82e23adf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:40:48.275447Z",
     "start_time": "2025-12-18T16:40:47.104991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base path of the unzipped movie dataset inside data/\n",
    "basepath = os.path.join(\"data\", \"aclImdb\")\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "\n",
    "rows = []   # temporary list to store [review_text, sentiment] pairs\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            rows.append([txt, labels[l]])  # store row instead of df.append\n",
    "            pbar.update()\n",
    "\n",
    "# Build the DataFrame once from the collected rows\n",
    "df = pd.DataFrame(rows, columns=['review', 'sentiment'])"
   ],
   "id": "7f21362b5d2997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using Nested For Loops, we itreated over the _train_ and the _test_ subdirectories in the main aclImdb directory and read the individual text files from _pos_ and _neg_ subdirectories that we eventually appended to the _df_ pandas _DataFrame_, together with the integer class label (1 = positive, 0 = negative )",
   "id": "d0b90c8f2e7b9b63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since the class labels in the assembled dataset are sorted, we will now shuffle _DataFrame_ using the **Permutation** function from _np.random_ submodule.\n",
    "This will be useful to split the dataset into training and testing datasets in the later parts of the projects, when we will stream the data from our local drive directly."
   ],
   "id": "c337e1fc2fe784d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:40:53.070080Z",
     "start_time": "2025-12-18T16:40:52.672019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ],
   "id": "b12544c142dd2f92",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_df = df.read_csv('movie_data.csv', encoding='utf-8')_\n",
    "\n",
    "No need to read the CSV again in the same session.\n",
    "Right now, at the end of the previous cell you already have:\n",
    "_df.to_csv('movie_data.csv', index=False, encoding='utf-8')_\n",
    "\n",
    "df is still in memory, so you can just do:\n",
    "_df.head()_"
   ],
   "id": "c759ca1d79e62367"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:40:57.645267Z",
     "start_time": "2025-12-18T16:40:57.639756Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(5)",
   "id": "1f99dba23d90eac9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "19602  OK... so... I really like Kris Kristofferson a...          0\n",
       "45519  ***SPOILER*** Do not read this, if you think a...          0\n",
       "25747  hi for all the people who have seen this wonde...          1\n",
       "42642  I recently bought the DVD, forgetting just how...          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introducing the Bag-Of-Words Model\n",
    "The idea behind the **Bag-Of-Words** is quiet simple\n",
    "1. We create a vocabulearly of unique tokens- E.g. words from the entire set of documnets\n",
    "2. We construct a feature vector from each document that contains the counts of how often each word occurs in a particular document."
   ],
   "id": "fa3e1228a5d50edf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transforming Words into Feature Vectors",
   "id": "10eed2eee6dddbfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this step, we use CountVectorizer to build a Bag-of-Words representation of a small corpus.\n",
    "The vectorizer first learns a vocabulary of all unique tokens in the four example sentences (count.vocabulary_).\n",
    "Then, fit_transform converts each sentence into a row of the document–term matrix, where each column corresponds to a word in the vocabulary and each entry stores how often that word appears in the sentence.\n",
    "The result (bag.toarray()) is a numerical matrix that we can feed into machine-learning models.\n"
   ],
   "id": "6bbd9e8ddb297d46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:03.715237Z",
     "start_time": "2025-12-18T16:41:03.349463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array(\n",
    "    ['The sun is shining',\n",
    "     'The weather is sweet',\n",
    "     'The sun is shining, the weather is sweet',\n",
    "     'and one and one is two'\n",
    "     ]\n",
    ")\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)"
   ],
   "id": "5cde56c036153439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:06.978576Z",
     "start_time": "2025-12-18T16:41:06.976053Z"
    }
   },
   "cell_type": "code",
   "source": "print(bag.toarray())",
   "id": "94a11beffeeeb014",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [0 2 0 1 1 1 2 0 1]\n",
      " [2 1 2 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Raw term frequencies\n",
    "\n",
    "In the bag-of-words model, *raw term frequency* for a word is simply the **number of times that word appears in a document**, without any extra scaling or weighting.\n",
    "\n",
    "- For each document, we build a vocabulary of all unique tokens (words).\n",
    "- For every word in that vocabulary, we count how many times it occurs in the document.\n",
    "- These counts form the feature vector for the document (one dimension per word).\n",
    "\n",
    "Example:\n",
    "If the document is:\n",
    "> \"the sun is shining, the weather is sweet\"\n",
    "\n",
    "and our vocabulary includes `[\"the\", \"sun\", \"is\", \"shining\", \"weather\", \"sweet\"]`, then the raw term frequencies are:\n",
    "\n",
    "- `the` → 2\n",
    "- `sun` → 1\n",
    "- `is` → 1\n",
    "- `shining` → 1\n",
    "- `weather` → 1\n",
    "- `sweet` → 1\n",
    "\n",
    "No normalization (like dividing by document length) and no weighting (like TF–IDF) is applied here — we are just using **plain counts**."
   ],
   "id": "75632a2d41b2a4d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### N-gram Models (Unigrams, Bigrams, Trigrams)\n",
    "\n",
    "In the previous section, we built a **bag-of-words** model using **raw term frequencies**.\n",
    "That model was based on **unigrams**, i.e. single words.\n",
    "\n",
    "An **n-gram** is defined as a sequence of *n* consecutive tokens (usually words) from a text:\n",
    "\n",
    "- **Unigram (1-gram)**: sequences of length 1\n",
    "  - Example: `\"the weather is sweet\"`\n",
    "    → `[\"the\", \"weather\", \"is\", \"sweet\"]`\n",
    "- **Bigram (2-gram)**: sequences of length 2\n",
    "  - Example: `\"the weather is sweet\"`\n",
    "    → `[\"the weather\", \"weather is\", \"is sweet\"]`\n",
    "- **Trigram (3-gram)**: sequences of length 3\n",
    "  - Example: `\"the weather is sweet\"`\n",
    "    → `[\"the weather is\", \"weather is sweet\"]`\n",
    "\n",
    "A unigram bag-of-words model ignores word order and only uses **individual word counts** as features.\n",
    "By contrast, **n-gram models** (with n ≥ 2) can capture short-range context and common phrases, such as:\n",
    "\n",
    "- sentiment phrases like `\"not good\"`, `\"very bad\"`, `\"really great\"`\n",
    "- named entities like `\"New York\"`, `\"Los Angeles\"`\n",
    "\n",
    "This makes n-gram features particularly useful in **sentiment analysis** and other NLP tasks where local word order carries important meaning."
   ],
   "id": "13bc3105424be888"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Assessing Word Relevancy with TF–IDF\n",
    "\n",
    "Bag-of-words and n-gram models give us **raw term frequencies** (how many times each word or phrase appears in a document). However, frequent words are not always informative. For example, words like “the”, “is”, or “movie” may appear in almost every review, regardless of sentiment.\n",
    "\n",
    "To capture **how relevant a word is for a specific document**, we use **TF–IDF (Term Frequency–Inverse Document Frequency)**.\n",
    "\n",
    "- **Term Frequency (TF)**\n",
    "  Measures how often a term appears in a single document.\n",
    "  Higher TF → the term is more important *within that document*.\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**\n",
    "  Measures how common or rare a term is across the whole collection of documents.\n",
    "  - Terms that appear in **many** documents (e.g., “the”) get a **low** IDF.\n",
    "  - Terms that appear in **fewer** documents (e.g., “excellent”, “terrible”) get a **higher** IDF.\n",
    "\n",
    "- **TF–IDF score**\n",
    "  TF–IDF combines these two ideas:\n",
    "\n",
    "  > **TF–IDF(term, document) = TF(term in this document) × IDF(term over all documents)**\n",
    "\n",
    "  Intuition:\n",
    "  - A term gets a **high TF–IDF** score if\n",
    "    - it appears frequently in this document (**high TF**), and\n",
    "    - it does **not** appear in most other documents (**high IDF**).\n",
    "  - Very common words across the corpus get **low TF–IDF** scores, even if they appear often in a document.\n",
    "\n",
    "In practice, TF–IDF helps the model focus on **discriminative words**, such as “excellent”, “boring”, “waste”, or “masterpiece”, which are more useful for tasks like **sentiment analysis** than very common function words."
   ],
   "id": "dc9c377f5ffea187"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scikit-Learn library implememts yet another transformer, the _TfidfTransformer_ class, which takes in raw term frequencies from the _CountVectorize_ class as input and transforms them into tf-idf",
   "id": "cc0be3efa325fe29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:12.348170Z",
     "start_time": "2025-12-18T16:41:12.343001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# counts from CountVectorizer\n",
    "counts = count.fit_transform(docs)\n",
    "\n",
    "# transform counts → tf–idf\n",
    "tfidf_matrix = tfidf.fit_transform(counts)\n",
    "\n",
    "print(tfidf_matrix.toarray())"
   ],
   "id": "db337dba274407eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.38 0.   0.57 0.57 0.   0.46 0.   0.  ]\n",
      " [0.   0.38 0.   0.   0.   0.57 0.46 0.   0.57]\n",
      " [0.   0.46 0.   0.35 0.35 0.35 0.56 0.   0.35]\n",
      " [0.66 0.17 0.66 0.   0.   0.   0.   0.33 0.  ]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cleaning Text Data\n",
    "\n",
    "Before we build our Bag-of-Words and TF–IDF models, we first need to clean the raw text.\n",
    "Real-world reviews contain a lot of “noise” such as HTML tags, punctuation, numbers,\n",
    "and inconsistent casing. If we feed this directly into the model, the vocabulary\n",
    "becomes messy and the model wastes capacity on useless tokens.\n",
    "\n",
    "In this project, our basic text-cleaning pipeline will:\n",
    "\n",
    "1. **Normalize the text**\n",
    "   - Convert everything to lowercase\n",
    "   - Remove HTML tags and line breaks\n",
    "\n",
    "2. **Remove unwanted characters**\n",
    "   - Strip punctuation, numbers, and other non-word symbols\n",
    "   - Collapse multiple spaces into a single space\n",
    "\n",
    "3. **Tokenize and filter**\n",
    "   - Split the cleaned text into individual tokens (words)\n",
    "   - Optionally remove stop words (very common words like *the*, *is*, *and*)\n",
    "\n",
    "After this preprocessing step, each review is reduced to a cleaner sequence of words,\n",
    "which we then feed into the Bag-of-Words / TF–IDF pipeline."
   ],
   "id": "82cdab0a41432e54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now remove all punctuations marks except for emoticon characters, such as :), since those are useful for sentiment analysis. To accomplish this task, we will use the Python's **Regular Expression (regex)** library, _re_",
   "id": "ba40074a20dec3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:16.431886Z",
     "start_time": "2025-12-18T16:41:16.429004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # extract emoticons like :) ;-) :D etc.\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "\n",
    "    # remove non-word chars, lowercase, then append emoticons (without '-')\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower())\n",
    "    text = text + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "\n",
    "    return text"
   ],
   "id": "432152cdbf05bbc7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Note: Regex backslashes and `SyntaxWarning: invalid escape sequence`\n",
    "\n",
    "**What’s happening?**\n",
    "\n",
    "- Our regex patterns contain things like `\\)` and `\\W`.\n",
    "  - In **regex**, these are valid:\n",
    "    - `\\)` means “literal `)`”\n",
    "    - `\\W` means “non-word character”\n",
    "- But in **normal Python strings**, a backslash starts an *escape sequence* (like `\\n`, `\\t`).\n",
    "  - `\\)` and `\\W` are **not** valid Python string escapes.\n",
    "  - Python still runs the code, but it shows warnings like:\n",
    "    `SyntaxWarning: invalid escape sequence '\\)'` and `'\\W'`.\n",
    "\n",
    "**Why this matters**\n",
    "\n",
    "- The regex logic is correct, but the way it’s written as a Python string is noisy.\n",
    "- These warnings can hide real problems later, so it’s good practice to fix them.\n",
    "\n",
    "**Fix**\n",
    "\n",
    "- Make regex patterns **raw strings** so backslashes are passed directly to the regex engine.\n",
    "- Use the `r\"\"` prefix:\n",
    "\n",
    "```python\n",
    "# bad (will raise SyntaxWarning)\n",
    "re.findall('(?::|;|=)(?:-)?(?:\\)|$begin:math:text$\\|D\\|P\\)\\'\\, text\\)\n",
    "re\\.sub\\(\\'\\[\\\\W\\]\\+\\'\\, \\' \\'\\, text\\.lower\\(\\)\\)\n",
    "\n",
    "\\# good \\(raw strings\\, no warnings\\)\n",
    "re\\.findall\\(r\\'\\(\\?\\:\\:\\|\\;\\|\\=\\)\\(\\?\\:\\-\\)\\?\\(\\?\\:$end:math:text$|\\(|D|P)', text)\n",
    "re.sub(r'[\\W]+', ' ', text.lower())"
   ],
   "id": "d093a7e40b130e43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:21.103890Z",
     "start_time": "2025-12-18T16:41:21.100034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lets confirm that our preprocessor works correctly\n",
    "preprocessor(df.loc[0, 'review'] [-50:])"
   ],
   "id": "693373a7b0d0cc9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and i suggest that you go see it before you judge  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:23.292407Z",
     "start_time": "2025-12-18T16:41:23.289694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Another one\n",
    "preprocessor(\"</a> This :) is :( a test :-)!\")"
   ],
   "id": "c3f78ebbd20211aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this is a test  :) :( :)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:27.595563Z",
     "start_time": "2025-12-18T16:41:25.969794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets apply our preprocessor function to all the movie reviews in our DataFrame\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ],
   "id": "bce96ec0f629f6bc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cleaning Text Data\n",
    "\n",
    "Before building our Bag-of-Words and TF–IDF models, we first normalize the raw text.\n",
    "\n",
    "Our `preprocessor` function does three main things:\n",
    "\n",
    "1. **Remove HTML tags and punctuation**\n",
    "   We strip out HTML markup (e.g. `<br />`) and most punctuation characters so that they do not become separate “words” in our vocabulary.\n",
    "\n",
    "2. **Preserve emoticons**\n",
    "   Simple emoticons such as `:)`, `:(`, `:-)` are extracted using a regular expression and re-attached to the cleaned text.\n",
    "   These patterns often carry strong sentiment and are therefore useful features.\n",
    "\n",
    "3. **Lowercase everything**\n",
    "   We convert the text to lowercase so that words like `Good`, `GOOD`, and `good` are treated as the same token.\n",
    "\n",
    "After preprocessing, each review is a cleaner, more uniform string that is easier to tokenize and vectorize. This reduces noise in the feature space and helps the classifier focus on actual sentiment patterns instead of formatting differences."
   ],
   "id": "7247c201b045050b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:30.858746Z",
     "start_time": "2025-12-18T16:41:30.856029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ],
   "id": "8d0d7b166bfee71e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using NLTK (Natural Language Toolkit)\n",
    "\n",
    "In this section we introduce **NLTK – Natural Language Toolkit**, a popular Python library for basic NLP tasks.\n",
    "\n",
    "At a high level, NLTK gives us:\n",
    "\n",
    "- **Tokenization**\n",
    "  Functions to split raw text into sentences or words.\n",
    "  Example:\n",
    "  `\"This is a test.\"` → `[\"This\", \"is\", \"a\", \"test\", \".\"]`\n",
    "\n",
    "- **Stop word lists**\n",
    "  Built-in lists of very common words such as *the, and, is* that usually do not carry much meaning for tasks like sentiment analysis.\n",
    "  We can remove these tokens to reduce noise.\n",
    "\n",
    "- **Stemming and lemmatization**\n",
    "  Tools to reduce different word forms to a common base:\n",
    "  - *Stemming* (e.g. Porter stemmer) cuts words down to a root:\n",
    "    `\"running\", \"runs\", \"ran\"` → `\"run\"`\n",
    "  - *Lemmatization* uses vocabulary and grammar rules to map words to a canonical form:\n",
    "    `\"mice\"` → `\"mouse\"`, `\"better\"` → `\"good\"`\n",
    "\n",
    "In our IMDB sentiment project we mainly use NLTK to:\n",
    "\n",
    "1. Build a **smarter tokenizer** than just `text.split()`.\n",
    "2. Optionally **remove stop words** that do not help the classifier.\n",
    "3. Optionally **stem** words so that different forms (e.g. *run, running, runs*) are treated as the same feature.\n",
    "\n",
    "This improves our Bag-of-Words and TF–IDF representations, because the model focuses on the core meaning of the text instead of superficial differences in capitalization, punctuation, or verb forms."
   ],
   "id": "28077056c780dd49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Porter Stemmer Algorithm\n",
    "\n",
    "The **Porter stemmer** is a classic rule-based algorithm for **stemming** English words.\n",
    "Stemming means reducing different word forms to a simpler, common **stem** by stripping off\n",
    "frequent suffixes.\n",
    "\n",
    "The Porter stemmer works in several steps, each applying rules such as:\n",
    "\n",
    "- `caresses → caress` (remove *es*)\n",
    "- `ponies → poni` (replace *ies* with *i*)\n",
    "- `caressed → caress` (remove *ed*)\n",
    "- `hopping → hop`, `hoped → hope`\n",
    "- `relational → relat`, `conditional → condit`\n",
    "\n",
    "The resulting stems are not always valid English words (e.g. *relat*, *studi*), but they are\n",
    "**consistent**, which is what we need for Bag-of-Words / TF–IDF features.\n",
    "\n",
    "In the IMDB sentiment project we use the Porter stemmer to:\n",
    "\n",
    "- collapse different inflected forms of a word (e.g. *run, runs, running, ran*) into one stem,\n",
    "- reduce the size of the vocabulary,\n",
    "- make the model focus on the underlying concepts rather than small spelling variations."
   ],
   "id": "8e2e56c61a140795"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:34.092385Z",
     "start_time": "2025-12-18T16:41:33.931960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ],
   "id": "184fd52a7656e342",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stop-Word Removal\n",
    "\n",
    "**Stop words** are very common words such as *the, a, an, is, was, and, or, to, of, in* that usually\n",
    "do not carry much content information. In a Bag-of-Words model with **raw** or **normalized term\n",
    "frequencies (tf)** these words:\n",
    "\n",
    "- appear in almost every document,\n",
    "- produce very high counts,\n",
    "- increase the dimensionality of the feature space,\n",
    "- and add little useful signal for classification.\n",
    "\n",
    "Therefore, stop-word removal is especially helpful when we work with tf-based features.\n",
    "\n",
    "When we use **TF–IDF**, extremely frequent words automatically receive a low weight because their\n",
    "inverse document frequency (IDF) is small. In that case, stop-word removal is less critical but\n",
    "still useful to reduce noise and vocabulary size.\n",
    "\n",
    "For sentiment analysis we usually *do not* remove negation words such as *not, no, never*, since\n",
    "they can completely change the polarity of a sentence (e.g. “good” vs “not good”). We typically\n",
    "start from a standard stop-word list and then customize it to keep important tokens like negations."
   ],
   "id": "75fd00776f662bde"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:37.567132Z",
     "start_time": "2025-12-18T16:41:37.563754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(len(ENGLISH_STOP_WORDS))\n",
    "list(ENGLISH_STOP_WORDS)[:20]"
   ],
   "id": "c57dfc6d0474e24c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['own',\n",
       " 'we',\n",
       " 'else',\n",
       " 'then',\n",
       " 'whatever',\n",
       " 'yourself',\n",
       " 'mostly',\n",
       " 'who',\n",
       " 'often',\n",
       " 'namely',\n",
       " 'those',\n",
       " 'also',\n",
       " 'front',\n",
       " 'which',\n",
       " 'sincere',\n",
       " 'few',\n",
       " 'fill',\n",
       " 'an',\n",
       " 'forty',\n",
       " 'enough']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stop-Word Removal\n",
    "\n",
    "**Stop words** are very common words such as *the, a, an, is, was, and, or, to, of* that usually do not carry much content information.\n",
    "\n",
    "In a Bag-of-Words model with **raw or normalized term frequencies (tf)** these words:\n",
    "\n",
    "- appear in almost every document,\n",
    "- produce very high counts,\n",
    "- increase the dimensionality of the feature space, and\n",
    "- add little useful signal for classification.\n",
    "\n",
    "Because of that, **stop-word removal** is especially helpful when we work with tf-based features.\n",
    "\n",
    "When we use **TF–IDF**, extremely frequent words automatically receive a low weight because their inverse document frequency (IDF) is small. In that case, stop-word removal is less critical, but it still helps reduce noise and vocabulary size.\n",
    "\n",
    "For **sentiment analysis** we usually **do not** remove negation words such as *not, no, never*, since they can completely change the polarity of a sentence (e.g. “good” vs “not good”). We typically start from a standard stop-word list and then customize it to keep important tokens like negations.\n",
    "\n",
    "---\n",
    "\n",
    "### Using Stop Words in This Project\n",
    "\n",
    "Instead of using NLTK’s stop words, we use the list that comes **built in with scikit-learn**:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# start from scikit-learn's default English stop words\n",
    "print(len(ENGLISH_STOP_WORDS))\n",
    "list(ENGLISH_STOP_WORDS)[:20]\n",
    "\n",
    "We can also customize this list, for example to keep negation words:\n",
    "custom_stopwords = ENGLISH_STOP_WORDS.difference({'not', 'no', 'never'})\n",
    "\n",
    "Then we plug this into our tokenizer + stemmer:\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter_no_stop(text):\n",
    "    return [\n",
    "        porter.stem(word)\n",
    "        for word in text.split()\n",
    "        if word.lower() not in custom_stopwords\n",
    "    ]\n",
    "This gives us stemmed tokens with most uninformative words removed, but keeps crucial negations.\n",
    "\n",
    "---\n",
    "\n",
    "### Why scikit-learn stop words worked but NLTK stop words did not\n",
    "\n",
    "When we tried to use NLTK:\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "we saw errors like:\n",
    "\t•\tSSL: CERTIFICATE_VERIFY_FAILED\n",
    "\t•\tLookupError: Resource 'stopwords' not found\n",
    "\n",
    "What happened:\n",
    "- nltk.download('stopwords') tries to download the stop-word corpus over HTTPS.\n",
    "- On this Mac, the HTTPS request failed because of a certificate verification issue (CERTIFICATE_VERIFY_FAILED).\n",
    "- As a result, the stopwords data never got saved to the local nltk_data folder.\n",
    "- Later, stopwords.words('english') looked for that file, couldn’t find it, and raised a LookupError.\n",
    "\n",
    "In contrast, scikit-learn’s ENGLISH_STOP_WORDS:\n",
    "\t•\tis bundled directly inside the scikit-learn package,\n",
    "\t•\tdoes not require any download or internet access,\n",
    "\t•\tso it works immediately with no SSL or lookup errors.\n",
    "\n",
    "Functionally, both NLTK and scikit-learn provide lists of common English stop words.\n",
    "For this IMDB sentiment project, using scikit-learn’s built-in list is perfectly fine and avoids the NLTK download issues on this machine."
   ],
   "id": "b110d43be1f8e129"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a Logistic Regression Model for Document Classification\n",
    "\n",
    "In this section we train a **logistic regression** classifier on the IMDB\n",
    "movie reviews using a Bag-of-Words / TF–IDF representation.\n",
    "\n",
    "### Why logistic regression?\n",
    "\n",
    "For binary text classification (positive vs negative review), logistic\n",
    "regression is a strong baseline:\n",
    "\n",
    "- It works very well with **high-dimensional sparse features** (like\n",
    "  bag-of-words).\n",
    "- The model is **linear**, easy to regularize, and relatively fast to train.\n",
    "- The output is a **probability** for each class, which is easy to interpret.\n",
    "\n",
    "### Train / test split\n",
    "\n",
    "We already built `movie_data.csv` with 50,000 reviews and labels:\n",
    "\n",
    "- 25,000 reviews for training\n",
    "- 25,000 reviews for testing\n",
    "\n",
    "We extract:\n",
    "\n",
    "- `X` = reviews (raw text)\n",
    "- `y` = sentiment labels (0 = negative, 1 = positive)\n",
    "\n",
    "Then we slice the first 25k as train and the rest as test, following the\n",
    "textbook.\n",
    "\n",
    "### Pipeline + GridSearchCV\n",
    "\n",
    "We build a scikit-learn `Pipeline` with two steps:\n",
    "\n",
    "1. **Vectorizer** (`TfidfVectorizer`):\n",
    "   - converts raw text → token counts → TF–IDF weights\n",
    "   - we will try different options for:\n",
    "     - `ngram_range` (unigrams vs bigrams)\n",
    "     - `stop_words` (use our stopwords or None)\n",
    "     - `tokenizer` (simple split vs Porter stemmer)\n",
    "\n",
    "2. **Classifier** (`LogisticRegression`):\n",
    "   - with L2 regularization\n",
    "   - hyperparameter `C` controls regularization strength\n",
    "     (small `C` = stronger regularization).\n",
    "\n",
    "We use **GridSearchCV** with 5-fold stratified cross-validation to search\n",
    "over combinations of:\n",
    "\n",
    "- BoW/TF–IDF parameters (ngrams, stop words, tokenizer, etc.)\n",
    "- Logistic regression parameters (`C`, penalty)\n",
    "\n",
    "The grid has two dictionaries:\n",
    "\n",
    "1. Standard TF–IDF settings (`use_idf=True`, `norm='l2'`).\n",
    "2. \"Raw tf\" style settings with `use_idf=False`, `smooth_idf=False`,\n",
    "   `norm=None`.\n",
    "\n",
    "This mirrors the idea from the chapter: **compare models based on pure term\n",
    "frequency vs TF–IDF**.\n",
    "\n",
    "We run `GridSearchCV` with:\n",
    "\n",
    "- `scoring='accuracy'`\n",
    "- `cv=5`\n",
    "- `n_jobs=-1` (use all cores)\n",
    "- `verbose=2` (show progress)\n",
    "\n",
    "After the search:\n",
    "\n",
    "- `gs_lr_tfidf.best_params_` gives the best combination of settings.\n",
    "- `gs_lr_tfidf.best_score_` gives mean CV accuracy.\n",
    "- `gs_lr_tfidf.best_estimator_` is the final trained pipeline.\n",
    "- We then evaluate on the 25k held-out test reviews to get **test accuracy**.\n",
    "\n",
    "The key takeaway: with a well-tuned logistic regression + TF–IDF, we can reach\n",
    "~90% accuracy on IMDB sentiment classification."
   ],
   "id": "a48898394b949143"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:41.261642Z",
     "start_time": "2025-12-18T16:41:41.010525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# 1. Load data (if you don't already have df in memory)\n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\", encoding=\"utf-8\")  # review, sentiment\n",
    "\n",
    "# 25k train / 25k test split as in the book\n",
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "\n",
    "X_test  = df.loc[25000:, 'review'].values\n",
    "y_test  = df.loc[25000:, 'sentiment'].values\n"
   ],
   "id": "4b73bedc4bbfb43a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:42.131281Z",
     "start_time": "2025-12-18T16:41:42.129223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Vectorizer + Logistic Regression pipeline\n",
    "\n",
    "\n",
    "# if you have your own preprocessor/tokenizers, import/define them here\n",
    "# from your earlier cells:\n",
    "# - preprocessor (clean HTML, emoticons, lowercasing, etc.)\n",
    "# - tokenizer (simple split with optional cleaning)\n",
    "# - tokenizer_porter (uses PorterStemmer)\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop = ENGLISH_STOP_WORDS  # our base stop-word list\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    strip_accents=None,\n",
    "    lowercase=False,          # we already handle casing in preprocessor\n",
    "    preprocessor=preprocessor # your custom preprocessor function\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    random_state=0,\n",
    "    solver=\"liblinear\"        # works well for small/medium text problems\n",
    ")\n",
    "\n",
    "lr_tfidf = Pipeline([\n",
    "    (\"vect\", tfidf),\n",
    "    (\"clf\", lr)\n",
    "])\n",
    "\n",
    "\n"
   ],
   "id": "ca33f9879bf52083",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:41:43.148301Z",
     "start_time": "2025-12-18T16:41:43.145417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Parameter grid\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"vect__ngram_range\": [(1, 1)],   # unigrams\n",
    "        \"vect__stop_words\":   [stop, None],\n",
    "        \"vect__tokenizer\":    [tokenizer, tokenizer_porter],\n",
    "        \"clf__penalty\":       [\"l2\"],\n",
    "        \"clf__C\":             [1.0, 10.0, 100.0]\n",
    "    },\n",
    "    {\n",
    "        \"vect__ngram_range\": [(1, 1)],\n",
    "        \"vect__stop_words\":   [stop, None],\n",
    "        \"vect__tokenizer\":    [tokenizer, tokenizer_porter],\n",
    "        \"vect__use_idf\":      [False],\n",
    "        \"vect__norm\":         [None],\n",
    "        \"clf__penalty\":       [\"l2\"],\n",
    "        \"clf__C\":             [1.0, 10.0, 100.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "62b37e811c5091da",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Regularization (short recap)\n",
    "\n",
    "In high-dimensional text data, a logistic regression model can easily overfit:\n",
    "it can give very large weights to some words or n-grams that only appear in the\n",
    "training set. **Regularization** is a way to control this complexity.\n",
    "\n",
    "We add a penalty on the size of the weight vector \\( w \\) to the loss\n",
    "function:\n",
    "\n",
    "- **L2 regularization (ridge)** uses \\( \\lambda \\sum_j w_j^2 \\).\n",
    "  This keeps many weights **small but non-zero**, which works very well with\n",
    "  Bag-of-Words / TF–IDF features.\n",
    "- **L1 regularization (lasso)** uses \\( \\lambda \\sum_j |w_j| \\).\n",
    "  This encourages **sparse** solutions where many weights are exactly zero,\n",
    "  which acts like an automatic feature selection.\n",
    "\n",
    "In scikit-learn’s `LogisticRegression` the strength of regularization is\n",
    "controlled by the parameter **C**, which is the **inverse** of \\(\\lambda\\):\n",
    "\n",
    "- small `C` → **strong** regularization (simpler model),\n",
    "- large `C` → **weak** regularization (more flexible model).\n",
    "\n",
    "In this chapter we tune `C` using cross-validation to find a good balance\n",
    "between **fitting the training data** and **generalizing to unseen reviews**."
   ],
   "id": "4985d09bd644629a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:43:00.325639Z",
     "start_time": "2025-12-18T16:41:46.764634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. GridSearchCV (5-fold stratified CV)\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(\n",
    "    estimator=lr_tfidf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print(\"CV Accuracy: %.3f\" % gs_lr_tfidf.best_score_)\n",
    "print(\"Best params:\", gs_lr_tfidf.best_params_)\n"
   ],
   "id": "6deb53686245989b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'a', 'describe', 'less', 'be', 'sixty', 'only', 'side', 'its', 'cant', 'otherwise', 'hereby', 'anyone', 'somehow', 'elsewhere', 'several', 'whenever', 'thick', 'he', 'because', 'back', 'due', 'none', 'two', 'thin', 'nevertheless', 'himself', 'neither', 'thereupon', 'so', 'made', 'becoming', 'else', 'name', 'another', 'after', 'some', 'last', 'noone', 'will', 'without', 'out', 'her', 'find', 'who', 'go', 'yourselves', 'done', 'have', 'and', 'is', 'even', 'could', 'too', 'somewhere', 'alone', 'are', 'on', 'afterwards', 'below', 'top', 'beyond', 'latter', 'anywhere', 'anyhow', 'cry', 'now', 'former', 'every', 'fifteen', 'bottom', 'nothing', 'also', 'mill', 'detail', 'into', 'part', 'seems', 'least', 'hers', 'toward', 'interest', 'me', 'amongst', 'should', 'indeed', 'five', 'ten', 'whom', 'except', 'eleven', 'meanwhile', 'fire', 'therefore', 'one', 'per', 'give', 'his', 'this', 'than', 'next', 'empty', 'whereafter', 'yourself', 'whence', 'whereby', 'there', 'hereupon', 'yours', 'wherein', 'couldnt', 'my', 'during', 'together', 'others', 'being', 'still', 'seem', 'it', 'serious', 'un', 'off', 'from', 'enough', 'eg', 'might', 'at', 'how', 'amount', 'third', 'along', 'anything', 'ours', 'may', 'de', 'take', 'by', 'has', 'front', 'can', 'was', 'both', 'etc', 'very', 'themselves', 'if', 'same', 'seeming', 'the', 'their', 'ever', 'before', 'further', 'whether', 'our', 'nor', 'everywhere', 'keep', 'move', 'inc', 'through', 'with', 'beside', 'please', 'up', 'these', 'perhaps', 'call', 'since', 'among', 'whatever', 'until', 'almost', 'thereafter', 'must', 'where', 'system', 'itself', 'what', 'all', 'mostly', 'i', 'such', 'hundred', 'but', 'hasnt', 'someone', 'they', 'sincere', 'full', 'then', 'wherever', 'most', 'besides', 'get', 'mine', 'found', 'twenty', 'which', 'above', 'moreover', 'therein', 'thereby', 'herself', 'more', 'us', 'co', 'whose', 'other', 'again', 'con', 'not', 'we', 'myself', 'to', 'upon', 'amoungst', 'via', 're', 'whither', 'however', 'that', 'twelve', 'rather', 'much', 'beforehand', 'either', 'sometimes', 'eight', 'them', 'would', 'nowhere', 'fill', 'why', 'were', 'nine', 'ie', 'you', 'am', 'fifty', 'or', 'under', 'around', 'put', 'herein', 'here', 'something', 'whereas', 'towards', 'hence', 'whole', 'see', 'ltd', 'had', 'four', 'thence', 'often', 'few', 'everything', 'onto', 'as', 'never', 'your', 'she', 'whereupon', 'own', 'him', 'any', 'hereafter', 'already', 'become', 'between', 'in', 'whoever', 'behind', 'of', 'cannot', 'anyway', 'within', 'many', 'been', 'becomes', 'against', 'forty', 'three', 'show', 'down', 'though', 'across', 'bill', 'namely', 'about', 'although', 'when', 'over', 'do', 'became', 'those', 'six', 'once', 'each', 'throughout', 'yet', 'sometime', 'first', 'formerly', 'seemed', 'an', 'for', 'everyone', 'no', 'while', 'latterly', 'nobody', 'ourselves', 'always', 'thru', 'thus', 'well'}), vect__tokenizer=<function tokenizer at 0x10efe3b00>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'someone', 'within', 'over', 'also', 'under', 'further', 'nine', 'were', 'least', 'call', 'full', 'formerly', 'beyond', 'still', 'since', 'then', 'three', 'cry', 'please', 'becoming', 'how', 'against', 'around', 'whose', 'nothing', 'eight', 'here', 'why', 'below', 'six', 'all', 'seem', 're', 'ie', 'themselves', 'without', 'de', 'whole', 'always', 'myself', 'rather', 'only', 'upon', 'itself', 'becomes', 'should', 'into', 'thru', 'him', 'whereby', 'get', 'us', 'them', 'but', 'could', 'at', 'seems', 'amount', 'might', 'done', 'this', 'beside', 'bottom', 'herein', 'except', 'couldnt', 'beforehand', 'detail', 'never', 'moreover', 'more', 'seeming', 'that', 'almost', 'whoever', 'whether', 'everyone', 'hereafter', 'otherwise', 'thus', 'empty', 'indeed', 'their', 'system', 'as', 'therein', 'my', 'was', 'nobody', 'sometime', 'fifty', 'latterly', 'go', 'will', 'find', 'me', 'twelve', 'yours', 'who', 'am', 'together', 'from', 'what', 'which', 'do', 'whither', 'five', 'because', 'up', 'would', 'during', 'i', 'anywhere', 'be', 'above', 'thereafter', 'many', 'whatever', 'a', 'any', 'cannot', 'put', 'towards', 'others', 'take', 'give', 'hasnt', 'your', 'anything', 'the', 'though', 'sincere', 'thin', 'whereupon', 'third', 'made', 'former', 'meanwhile', 'it', 'have', 'so', 'back', 'ten', 'mostly', 'etc', 'inc', 'toward', 'they', 'amoungst', 'first', 'she', 'next', 'eg', 'something', 'about', 'four', 'much', 'both', 'twenty', 'afterwards', 'when', 'between', 'see', 'there', 'alone', 'yourselves', 'hence', 'fifteen', 'elsewhere', 'seemed', 'his', 'fire', 'every', 'eleven', 'if', 'some', 'by', 'perhaps', 'anyhow', 'top', 'on', 'keep', 'somewhere', 'whom', 'we', 'or', 'same', 'two', 'among', 'in', 'very', 'whenever', 'fill', 'our', 'had', 'become', 'side', 'serious', 'wherein', 'nevertheless', 'therefore', 'few', 'noone', 'move', 'now', 'neither', 'thence', 'too', 'latter', 'those', 'anyway', 'is', 'once', 'mine', 'describe', 'well', 'can', 'less', 'whence', 'of', 'co', 'himself', 'through', 'onto', 'last', 'un', 'amongst', 'may', 'has', 'thereby', 'else', 'after', 'per', 'must', 'show', 'not', 'however', 'whereafter', 'being', 'con', 'most', 'somehow', 'via', 'mill', 'herself', 'own', 'cant', 'behind', 'until', 'along', 'are', 'hundred', 'to', 'besides', 'these', 'other', 'none', 'down', 'yourself', 'again', 'before', 'name', 'out', 'nor', 'found', 'off', 'namely', 'sixty', 'her', 'due', 'its', 'hereupon', 'throughout', 'no', 'each', 'enough', 'became', 'he', 'an', 'while', 'thereupon', 'wherever', 'one', 'bill', 'although', 'part', 'thick', 'and', 'hereby', 'even', 'everything', 'sometimes', 'such', 'across', 'yet', 'already', 'ltd', 'than', 'interest', 'several', 'been', 'you', 'often', 'nowhere', 'everywhere', 'ours', 'another', 'ever', 'ourselves', 'for', 'where', 'front', 'with', 'either', 'whereas', 'anyone', 'forty', 'hers'}), vect__tokenizer=<function tokenizer at 0x10f48fb00>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'less', 'once', 'anywhere', 'none', 'during', 'never', 'found', 'five', 'always', 'i', 'due', 'very', 'becoming', 'beforehand', 'much', 'now', 'done', 'while', 'ltd', 'along', 'eleven', 'it', 'etc', 'who', 'couldnt', 'keep', 'a', 'alone', 'hereupon', 'yourself', 'hers', 'we', 'twelve', 'hence', 'besides', 'hereafter', 'if', 'call', 'his', 'an', 'several', 'among', 'hundred', 'whereafter', 'bottom', 'six', 'those', 'he', 'therefore', 'up', 'together', 'most', 'that', 'yourselves', 'though', 'twenty', 'therein', 'whose', 'meanwhile', 'first', 'inc', 'indeed', 'into', 'except', 'detail', 'below', 'describe', 'whenever', 'themselves', 'thence', 'although', 'them', 'formerly', 'again', 'anyway', 'whither', 'wherever', 'upon', 'itself', 'herein', 'there', 'own', 'nine', 'against', 'been', 'myself', 'same', 'become', 'somehow', 'yours', 'last', 'forty', 'fifty', 'could', 'thereupon', 'give', 'above', 'least', 'at', 'system', 'be', 'eg', 'see', 'three', 'cry', 'noone', 'than', 'few', 'per', 'must', 'amongst', 'she', 'please', 'seeming', 'seemed', 'thin', 'some', 'out', 'am', 'already', 'whereas', 'thru', 'because', 'are', 'from', 'when', 'often', 'de', 'these', 'mill', 'all', 'made', 'without', 'as', 'take', 'co', 'for', 'anyhow', 'via', 'another', 'since', 'its', 'cannot', 'either', 'or', 'whence', 'their', 'why', 'fill', 'mostly', 'neither', 'by', 'put', 'sometime', 'someone', 'is', 'yet', 'somewhere', 'hereby', 'also', 'would', 'ten', 'else', 'whatever', 'still', 'go', 'to', 'the', 'not', 'something', 'which', 'sincere', 'with', 'her', 'con', 'whom', 'any', 'around', 'empty', 'nobody', 'many', 'moreover', 'in', 'interest', 'whereupon', 'nowhere', 'four', 'thereby', 'however', 'our', 'fire', 'within', 'have', 'herself', 'mine', 'latter', 'thus', 'nevertheless', 'latterly', 'whereby', 'next', 're', 'off', 'wherein', 'even', 'whoever', 'beside', 'two', 'seems', 'might', 'others', 'became', 'throughout', 'back', 'us', 'anyone', 'un', 'himself', 'everywhere', 'here', 'anything', 'ours', 'name', 'well', 'third', 'were', 'afterwards', 'every', 'until', 'how', 'they', 'part', 'ever', 'has', 'such', 'top', 'this', 'my', 'no', 'will', 'ie', 'but', 'both', 'onto', 'almost', 'nothing', 'further', 'get', 'side', 'toward', 'whole', 'thick', 'eight', 'on', 'nor', 'sometimes', 'show', 'beyond', 'only', 'over', 'through', 'whether', 'what', 'becomes', 'former', 'each', 'cant', 'perhaps', 'where', 'enough', 'namely', 'other', 'do', 'may', 'had', 'hasnt', 'me', 'full', 'otherwise', 'about', 'one', 'should', 'thereafter', 'fifteen', 'after', 'down', 'find', 'sixty', 'more', 'across', 'too', 'move', 'behind', 'so', 'everything', 'serious', 'front', 'amount', 'then', 'can', 'and', 'between', 'ourselves', 'was', 'bill', 'seem', 'you', 'being', 'him', 'of', 'rather', 'amoungst', 'everyone', 'before', 'elsewhere', 'under', 'your', 'towards'}), vect__tokenizer=<function tokenizer at 0x10eaf3b00>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'much', 'its', 'itself', 'such', 'thin', 'seeming', 'more', 'themselves', 'still', 'which', 'well', 'whence', 'please', 'herein', 'ltd', 'less', 'why', 'indeed', 'several', 'whereas', 'had', 'already', 'wherein', 'noone', 'serious', 'besides', 'nine', 'must', 'anywhere', 'ten', 'three', 'hereafter', 'do', 'everything', 'co', 'anyhow', 'how', 'hereupon', 'throughout', 'off', 'thereafter', 'un', 'somewhere', 'could', 'any', 'you', 'ever', 'whenever', 'top', 'at', 'per', 'anything', 'however', 'against', 'from', 'eg', 'his', 'even', 'my', 'find', 'someone', 'but', 'both', 'twelve', 'have', 'sometime', 'should', 'who', 'up', 'six', 'con', 'last', 'when', 'though', 'everywhere', 'found', 'enough', 'cannot', 'few', 'whereby', 'behind', 'whither', 'due', 'seems', 'for', 'next', 'something', 'wherever', 'twenty', 'done', 'own', 'namely', 'keep', 'never', 'in', 'amount', 'without', 'inc', 'call', 'me', 'being', 'into', 'not', 'thereupon', 'bill', 'of', 'sixty', 'mill', 'nevertheless', 'before', 'every', 'nobody', 'etc', 'moreover', 'same', 'be', 'yet', 'because', 'nowhere', 'empty', 'latter', 'show', 'many', 'other', 'those', 'hence', 'across', 'most', 'herself', 'all', 'our', 'has', 'amoungst', 'see', 'either', 'whereupon', 'thence', 'another', 'name', 'take', 'detail', 'ourselves', 'whatever', 'became', 'whom', 'i', 'some', 'third', 'somehow', 'by', 'an', 'none', 'were', 'very', 'seem', 'she', 'describe', 'among', 'thick', 'them', 'first', 'these', 'anyone', 'us', 'side', 'thus', 'then', 'nor', 'ours', 'further', 'sincere', 'whoever', 'if', 'around', 'the', 'himself', 'after', 'together', 'so', 'alone', 'bottom', 'him', 'former', 'your', 'back', 'beside', 'over', 'out', 'except', 'couldnt', 'above', 'almost', 'two', 'hers', 'on', 'will', 'to', 'hereby', 'whether', 'until', 'front', 'perhaps', 'part', 'de', 'becoming', 'can', 'are', 'hundred', 'their', 'beyond', 'he', 'otherwise', 'thereby', 'or', 'down', 'as', 'else', 'full', 'upon', 'meanwhile', 'they', 'onto', 'mine', 'only', 'made', 'her', 'this', 'would', 'fire', 'eight', 'within', 'here', 'myself', 'go', 'between', 'may', 'thru', 'via', 'move', 'mostly', 'and', 'formerly', 'become', 'give', 'amongst', 'whereafter', 'sometimes', 'seemed', 'ie', 'others', 'yours', 'elsewhere', 'toward', 'during', 're', 'that', 'five', 'what', 'might', 'rather', 'about', 'yourselves', 'four', 'yourself', 'now', 'fill', 'again', 'since', 'while', 'there', 'fifty', 'although', 'get', 'always', 'than', 'a', 'afterwards', 'fifteen', 'with', 'least', 'am', 'therein', 'anyway', 'cant', 'hasnt', 'eleven', 'therefore', 'we', 'forty', 'is', 'nothing', 'everyone', 'latterly', 'where', 'often', 'system', 'becomes', 'along', 'one', 'cry', 'was', 'below', 'under', 'also', 'whose', 'been', 'each', 'towards', 'it', 'whole', 'beforehand', 'interest', 'through', 'once', 'neither', 'too', 'no', 'put'}), vect__tokenizer=<function tokenizer at 0x10efafb00>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x112a2bb00>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'found', 'moreover', 'either', 'thereupon', 'otherwise', 'somehow', 'ours', 'if', 'but', 'anywhere', 'becoming', 'hasnt', 'you', 'his', 'ten', 'against', 'move', 'such', 'again', 'along', 'couldnt', 'hereupon', 'even', 'etc', 'nine', 'am', 'should', 'amoungst', 'give', 'into', 'made', 'onto', 'and', 'down', 'these', 'per', 'five', 'mostly', 'too', 'system', 'however', 'this', 'my', 'others', 'herein', 'elsewhere', 'much', 'whole', 'up', 'myself', 'whom', 'nobody', 'are', 'mill', 'interest', 'mine', 'indeed', 'why', 'formerly', 'almost', 'cannot', 'among', 'some', 'first', 'enough', 'thence', 'its', 'under', 'thick', 'latter', 'fifteen', 'not', 'everyone', 'though', 'by', 'beforehand', 'sometimes', 'former', 'yourselves', 'thereafter', 'beyond', 'anyway', 'something', 'an', 'except', 'themselves', 'eleven', 'hers', 'itself', 'been', 'can', 'herself', 'neither', 'ltd', 'upon', 'less', 'namely', 'each', 'thin', 'while', 'four', 'sincere', 'due', 'seems', 'here', 'perhaps', 'ie', 'he', 'none', 'most', 'to', 'both', 'top', 'in', 'could', 'own', 'third', 'amongst', 'through', 'hereby', 'off', 'toward', 'anyone', 'seem', 'take', 'yourself', 'throughout', 'further', 'meanwhile', 'our', 'therefore', 'via', 'there', 'she', 'go', 'it', 'become', 'seemed', 'so', 'several', 'will', 'sometime', 'bottom', 'someone', 'ourselves', 'fill', 'because', 'full', 'during', 'they', 'i', 'show', 'whenever', 'me', 're', 'would', 'often', 'whence', 'last', 'twelve', 'part', 'only', 'thru', 'once', 'anyhow', 'two', 'has', 'fire', 'other', 'as', 'find', 'himself', 'above', 'nevertheless', 'everything', 'them', 'six', 'we', 'be', 'somewhere', 'another', 'around', 'least', 'well', 'whose', 'whether', 'being', 'describe', 'eight', 'whereupon', 'than', 'please', 'noone', 'when', 'every', 'out', 'or', 'beside', 'seeming', 'inc', 'afterwards', 'might', 'whatever', 'always', 'from', 'although', 'their', 'twenty', 'more', 'below', 'hence', 'yet', 'before', 'the', 'whereas', 'do', 'behind', 'became', 'whither', 'front', 'that', 'yours', 'him', 'done', 'get', 'three', 'back', 'until', 'thus', 'empty', 'across', 'everywhere', 'forty', 'her', 'call', 'never', 'side', 'with', 'also', 'is', 'else', 'must', 'becomes', 'latterly', 'bill', 'few', 'those', 'same', 'co', 'de', 'cry', 'keep', 'at', 'after', 'sixty', 'whereafter', 'towards', 'between', 'your', 'cant', 'all', 'since', 'whereby', 'alone', 'many', 'what', 'nothing', 'for', 'hereafter', 'wherever', 'one', 'where', 'put', 'were', 'serious', 'fifty', 'without', 'eg', 'detail', 'which', 'of', 'any', 'un', 'rather', 'already', 'besides', 'next', 'nowhere', 'name', 'on', 'thereby', 'who', 'within', 'wherein', 'still', 'us', 'a', 'then', 'now', 'have', 'had', 'hundred', 'together', 'therein', 'anything', 'ever', 'amount', 'nor', 'may', 'very', 'see', 'over', 'was', 'how', 'con', 'no', 'about', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x11394c720>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'four', 'please', 'hundred', 'may', 'rather', 'get', 'thick', 'to', 'several', 'first', 'and', 'sixty', 'wherein', 'then', 'less', 'thereafter', 'one', 'well', 'formerly', 'hence', 'everything', 'of', 'every', 'co', 'although', 'whither', 'others', 'least', 'ever', 'with', 'whereby', 'seem', 'cry', 'her', 'his', 'ourselves', 'this', 'all', 'from', 'upon', 'even', 'often', 'among', 'therefore', 'amongst', 'three', 'most', 'done', 'nothing', 'been', 'other', 'eight', 'you', 'am', 'name', 'so', 'our', 'never', 'or', 'becomes', 'since', 'whereafter', 'herself', 'during', 'sincere', 'over', 'more', 'anyone', 'if', 'will', 'hereby', 'thin', 'interest', 'whence', 'thru', 'ten', 'nowhere', 'in', 'the', 'last', 'twelve', 'thereby', 'were', 'towards', 'these', 'whoever', 'mine', 'along', 'itself', 'within', 'that', 'now', 'un', 'themselves', 'former', 'afterwards', 'have', 'is', 'ltd', 'fifteen', 'before', 'yourselves', 'show', 'somewhere', 'up', 'yourself', 'latterly', 'across', 'why', 'eleven', 'on', 'about', 'himself', 'detail', 'amount', 'though', 'who', 'your', 'latter', 'hasnt', 'off', 'mostly', 'serious', 'can', 'down', 'beyond', 'both', 'give', 'behind', 'many', 'here', 'per', 'had', 'anyway', 'found', 'could', 'are', 'move', 'ie', 'became', 'indeed', 'same', 'elsewhere', 'keep', 'must', 'might', 'six', 'no', 'him', 'between', 'moreover', 'whatever', 'still', 'whole', 'hereupon', 'someone', 'them', 'anyhow', 'after', 'thereupon', 'wherever', 'twenty', 'would', 'there', 'else', 'whereupon', 'beside', 'fill', 'sometime', 'once', 'yours', 'inc', 'which', 'not', 'seems', 'was', 'meanwhile', 'system', 'only', 'couldnt', 'thus', 'thence', 'hereafter', 'further', 'amoungst', 'few', 'together', 'everywhere', 'because', 'nine', 'eg', 'nor', 'yet', 'bottom', 'how', 'back', 'but', 'neither', 'bill', 'he', 'it', 'already', 'through', 'five', 'via', 'find', 'under', 'de', 'whose', 'be', 'ours', 'below', 'until', 'perhaps', 'me', 'those', 'con', 'forty', 'nevertheless', 'everyone', 'its', 'describe', 'become', 'always', 'part', 'very', 'somehow', 'none', 'see', 'at', 'otherwise', 'as', 'therein', 'something', 'full', 'third', 'empty', 'into', 'fifty', 'call', 'should', 'onto', 'cant', 'front', 'noone', 'we', 'where', 'myself', 'against', 'made', 'namely', 'has', 'anywhere', 'their', 'top', 'herein', 'for', 'whereas', 'hers', 'nobody', 'mill', 'cannot', 'another', 'when', 'throughout', 'fire', 'without', 'toward', 'such', 'own', 'sometimes', 'some', 'she', 'seeming', 'besides', 'than', 'around', 'an', 'they', 'my', 'alone', 'either', 'each', 'except', 'whether', 'next', 'put', 'however', 'i', 'two', 'us', 'above', 'whom', 'beforehand', 'by', 'much', 'also', 'too', 'whenever', 'enough', 'anything', 'a', 'seemed', 'becoming', 'almost', 'out', 'etc', 'being', 'go', 'due', 'again', 'while', 'what', 'take', 'any', 're', 'do', 'side'}), vect__tokenizer=<function tokenizer_porter at 0x112e70720>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x112b7b9c0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer_porter at 0x111354720>; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}), vect__tokenizer=<function tokenizer_porter at 0x10f238720>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'found', 'moreover', 'either', 'thereupon', 'otherwise', 'somehow', 'ours', 'if', 'but', 'anywhere', 'becoming', 'hasnt', 'you', 'his', 'ten', 'against', 'move', 'such', 'again', 'along', 'couldnt', 'hereupon', 'even', 'etc', 'nine', 'am', 'should', 'amoungst', 'give', 'into', 'made', 'onto', 'and', 'down', 'these', 'per', 'five', 'mostly', 'too', 'system', 'however', 'this', 'my', 'others', 'herein', 'elsewhere', 'much', 'whole', 'up', 'myself', 'whom', 'nobody', 'are', 'mill', 'interest', 'mine', 'indeed', 'why', 'formerly', 'almost', 'cannot', 'among', 'some', 'first', 'enough', 'thence', 'its', 'under', 'thick', 'latter', 'fifteen', 'not', 'everyone', 'though', 'by', 'beforehand', 'sometimes', 'former', 'yourselves', 'thereafter', 'beyond', 'anyway', 'something', 'an', 'except', 'themselves', 'eleven', 'hers', 'itself', 'been', 'can', 'herself', 'neither', 'ltd', 'upon', 'less', 'namely', 'each', 'thin', 'while', 'four', 'sincere', 'due', 'seems', 'here', 'perhaps', 'ie', 'he', 'none', 'most', 'to', 'both', 'top', 'in', 'could', 'own', 'third', 'amongst', 'through', 'hereby', 'off', 'toward', 'anyone', 'seem', 'take', 'yourself', 'throughout', 'further', 'meanwhile', 'our', 'therefore', 'via', 'there', 'she', 'go', 'it', 'become', 'seemed', 'so', 'several', 'will', 'sometime', 'bottom', 'someone', 'ourselves', 'fill', 'because', 'full', 'during', 'they', 'i', 'show', 'whenever', 'me', 're', 'would', 'often', 'whence', 'last', 'twelve', 'part', 'only', 'thru', 'once', 'anyhow', 'two', 'has', 'fire', 'other', 'as', 'find', 'himself', 'above', 'nevertheless', 'everything', 'them', 'six', 'we', 'be', 'somewhere', 'another', 'around', 'least', 'well', 'whose', 'whether', 'being', 'describe', 'eight', 'whereupon', 'than', 'please', 'noone', 'when', 'every', 'out', 'or', 'beside', 'seeming', 'inc', 'afterwards', 'might', 'whatever', 'always', 'from', 'although', 'their', 'twenty', 'more', 'below', 'hence', 'yet', 'before', 'the', 'whereas', 'do', 'behind', 'became', 'whither', 'front', 'that', 'yours', 'him', 'done', 'get', 'three', 'back', 'until', 'thus', 'empty', 'across', 'everywhere', 'forty', 'her', 'call', 'never', 'side', 'with', 'also', 'is', 'else', 'must', 'becomes', 'latterly', 'bill', 'few', 'those', 'same', 'co', 'de', 'cry', 'keep', 'at', 'after', 'sixty', 'whereafter', 'towards', 'between', 'your', 'cant', 'all', 'since', 'whereby', 'alone', 'many', 'what', 'nothing', 'for', 'hereafter', 'wherever', 'one', 'where', 'put', 'were', 'serious', 'fifty', 'without', 'eg', 'detail', 'which', 'of', 'any', 'un', 'rather', 'already', 'besides', 'next', 'nowhere', 'name', 'on', 'thereby', 'who', 'within', 'wherein', 'still', 'us', 'a', 'then', 'now', 'have', 'had', 'hundred', 'together', 'therein', 'anything', 'ever', 'amount', 'nor', 'may', 'very', 'see', 'over', 'was', 'how', 'con', 'no', 'about', 'whoever'}), vect__tokenizer=<function tokenizer at 0x10b18e3e0>; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer_porter at 0x10fc1c720>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer at 0x108baa3e0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'found', 'moreover', 'either', 'thereupon', 'otherwise', 'somehow', 'ours', 'if', 'but', 'anywhere', 'becoming', 'hasnt', 'you', 'his', 'ten', 'against', 'move', 'such', 'again', 'along', 'couldnt', 'hereupon', 'even', 'etc', 'nine', 'am', 'should', 'amoungst', 'give', 'into', 'made', 'onto', 'and', 'down', 'these', 'per', 'five', 'mostly', 'too', 'system', 'however', 'this', 'my', 'others', 'herein', 'elsewhere', 'much', 'whole', 'up', 'myself', 'whom', 'nobody', 'are', 'mill', 'interest', 'mine', 'indeed', 'why', 'formerly', 'almost', 'cannot', 'among', 'some', 'first', 'enough', 'thence', 'its', 'under', 'thick', 'latter', 'fifteen', 'not', 'everyone', 'though', 'by', 'beforehand', 'sometimes', 'former', 'yourselves', 'thereafter', 'beyond', 'anyway', 'something', 'an', 'except', 'themselves', 'eleven', 'hers', 'itself', 'been', 'can', 'herself', 'neither', 'ltd', 'upon', 'less', 'namely', 'each', 'thin', 'while', 'four', 'sincere', 'due', 'seems', 'here', 'perhaps', 'ie', 'he', 'none', 'most', 'to', 'both', 'top', 'in', 'could', 'own', 'third', 'amongst', 'through', 'hereby', 'off', 'toward', 'anyone', 'seem', 'take', 'yourself', 'throughout', 'further', 'meanwhile', 'our', 'therefore', 'via', 'there', 'she', 'go', 'it', 'become', 'seemed', 'so', 'several', 'will', 'sometime', 'bottom', 'someone', 'ourselves', 'fill', 'because', 'full', 'during', 'they', 'i', 'show', 'whenever', 'me', 're', 'would', 'often', 'whence', 'last', 'twelve', 'part', 'only', 'thru', 'once', 'anyhow', 'two', 'has', 'fire', 'other', 'as', 'find', 'himself', 'above', 'nevertheless', 'everything', 'them', 'six', 'we', 'be', 'somewhere', 'another', 'around', 'least', 'well', 'whose', 'whether', 'being', 'describe', 'eight', 'whereupon', 'than', 'please', 'noone', 'when', 'every', 'out', 'or', 'beside', 'seeming', 'inc', 'afterwards', 'might', 'whatever', 'always', 'from', 'although', 'their', 'twenty', 'more', 'below', 'hence', 'yet', 'before', 'the', 'whereas', 'do', 'behind', 'became', 'whither', 'front', 'that', 'yours', 'him', 'done', 'get', 'three', 'back', 'until', 'thus', 'empty', 'across', 'everywhere', 'forty', 'her', 'call', 'never', 'side', 'with', 'also', 'is', 'else', 'must', 'becomes', 'latterly', 'bill', 'few', 'those', 'same', 'co', 'de', 'cry', 'keep', 'at', 'after', 'sixty', 'whereafter', 'towards', 'between', 'your', 'cant', 'all', 'since', 'whereby', 'alone', 'many', 'what', 'nothing', 'for', 'hereafter', 'wherever', 'one', 'where', 'put', 'were', 'serious', 'fifty', 'without', 'eg', 'detail', 'which', 'of', 'any', 'un', 'rather', 'already', 'besides', 'next', 'nowhere', 'name', 'on', 'thereby', 'who', 'within', 'wherein', 'still', 'us', 'a', 'then', 'now', 'have', 'had', 'hundred', 'together', 'therein', 'anything', 'ever', 'amount', 'nor', 'may', 'very', 'see', 'over', 'was', 'how', 'con', 'no', 'about', 'whoever'}), vect__tokenizer=<function tokenizer at 0x1133c7740>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'four', 'please', 'hundred', 'may', 'rather', 'get', 'thick', 'to', 'several', 'first', 'and', 'sixty', 'wherein', 'then', 'less', 'thereafter', 'one', 'well', 'formerly', 'hence', 'everything', 'of', 'every', 'co', 'although', 'whither', 'others', 'least', 'ever', 'with', 'whereby', 'seem', 'cry', 'her', 'his', 'ourselves', 'this', 'all', 'from', 'upon', 'even', 'often', 'among', 'therefore', 'amongst', 'three', 'most', 'done', 'nothing', 'been', 'other', 'eight', 'you', 'am', 'name', 'so', 'our', 'never', 'or', 'becomes', 'since', 'whereafter', 'herself', 'during', 'sincere', 'over', 'more', 'anyone', 'if', 'will', 'hereby', 'thin', 'interest', 'whence', 'thru', 'ten', 'nowhere', 'in', 'the', 'last', 'twelve', 'thereby', 'were', 'towards', 'these', 'whoever', 'mine', 'along', 'itself', 'within', 'that', 'now', 'un', 'themselves', 'former', 'afterwards', 'have', 'is', 'ltd', 'fifteen', 'before', 'yourselves', 'show', 'somewhere', 'up', 'yourself', 'latterly', 'across', 'why', 'eleven', 'on', 'about', 'himself', 'detail', 'amount', 'though', 'who', 'your', 'latter', 'hasnt', 'off', 'mostly', 'serious', 'can', 'down', 'beyond', 'both', 'give', 'behind', 'many', 'here', 'per', 'had', 'anyway', 'found', 'could', 'are', 'move', 'ie', 'became', 'indeed', 'same', 'elsewhere', 'keep', 'must', 'might', 'six', 'no', 'him', 'between', 'moreover', 'whatever', 'still', 'whole', 'hereupon', 'someone', 'them', 'anyhow', 'after', 'thereupon', 'wherever', 'twenty', 'would', 'there', 'else', 'whereupon', 'beside', 'fill', 'sometime', 'once', 'yours', 'inc', 'which', 'not', 'seems', 'was', 'meanwhile', 'system', 'only', 'couldnt', 'thus', 'thence', 'hereafter', 'further', 'amoungst', 'few', 'together', 'everywhere', 'because', 'nine', 'eg', 'nor', 'yet', 'bottom', 'how', 'back', 'but', 'neither', 'bill', 'he', 'it', 'already', 'through', 'five', 'via', 'find', 'under', 'de', 'whose', 'be', 'ours', 'below', 'until', 'perhaps', 'me', 'those', 'con', 'forty', 'nevertheless', 'everyone', 'its', 'describe', 'become', 'always', 'part', 'very', 'somehow', 'none', 'see', 'at', 'otherwise', 'as', 'therein', 'something', 'full', 'third', 'empty', 'into', 'fifty', 'call', 'should', 'onto', 'cant', 'front', 'noone', 'we', 'where', 'myself', 'against', 'made', 'namely', 'has', 'anywhere', 'their', 'top', 'herein', 'for', 'whereas', 'hers', 'nobody', 'mill', 'cannot', 'another', 'when', 'throughout', 'fire', 'without', 'toward', 'such', 'own', 'sometimes', 'some', 'she', 'seeming', 'besides', 'than', 'around', 'an', 'they', 'my', 'alone', 'either', 'each', 'except', 'whether', 'next', 'put', 'however', 'i', 'two', 'us', 'above', 'whom', 'beforehand', 'by', 'much', 'also', 'too', 'whenever', 'enough', 'anything', 'a', 'seemed', 'becoming', 'almost', 'out', 'etc', 'being', 'go', 'due', 'again', 'while', 'what', 'take', 'any', 're', 'do', 'side'}), vect__tokenizer=<function tokenizer at 0x10a72a3e0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'found', 'moreover', 'either', 'thereupon', 'otherwise', 'somehow', 'ours', 'if', 'but', 'anywhere', 'becoming', 'hasnt', 'you', 'his', 'ten', 'against', 'move', 'such', 'again', 'along', 'couldnt', 'hereupon', 'even', 'etc', 'nine', 'am', 'should', 'amoungst', 'give', 'into', 'made', 'onto', 'and', 'down', 'these', 'per', 'five', 'mostly', 'too', 'system', 'however', 'this', 'my', 'others', 'herein', 'elsewhere', 'much', 'whole', 'up', 'myself', 'whom', 'nobody', 'are', 'mill', 'interest', 'mine', 'indeed', 'why', 'formerly', 'almost', 'cannot', 'among', 'some', 'first', 'enough', 'thence', 'its', 'under', 'thick', 'latter', 'fifteen', 'not', 'everyone', 'though', 'by', 'beforehand', 'sometimes', 'former', 'yourselves', 'thereafter', 'beyond', 'anyway', 'something', 'an', 'except', 'themselves', 'eleven', 'hers', 'itself', 'been', 'can', 'herself', 'neither', 'ltd', 'upon', 'less', 'namely', 'each', 'thin', 'while', 'four', 'sincere', 'due', 'seems', 'here', 'perhaps', 'ie', 'he', 'none', 'most', 'to', 'both', 'top', 'in', 'could', 'own', 'third', 'amongst', 'through', 'hereby', 'off', 'toward', 'anyone', 'seem', 'take', 'yourself', 'throughout', 'further', 'meanwhile', 'our', 'therefore', 'via', 'there', 'she', 'go', 'it', 'become', 'seemed', 'so', 'several', 'will', 'sometime', 'bottom', 'someone', 'ourselves', 'fill', 'because', 'full', 'during', 'they', 'i', 'show', 'whenever', 'me', 're', 'would', 'often', 'whence', 'last', 'twelve', 'part', 'only', 'thru', 'once', 'anyhow', 'two', 'has', 'fire', 'other', 'as', 'find', 'himself', 'above', 'nevertheless', 'everything', 'them', 'six', 'we', 'be', 'somewhere', 'another', 'around', 'least', 'well', 'whose', 'whether', 'being', 'describe', 'eight', 'whereupon', 'than', 'please', 'noone', 'when', 'every', 'out', 'or', 'beside', 'seeming', 'inc', 'afterwards', 'might', 'whatever', 'always', 'from', 'although', 'their', 'twenty', 'more', 'below', 'hence', 'yet', 'before', 'the', 'whereas', 'do', 'behind', 'became', 'whither', 'front', 'that', 'yours', 'him', 'done', 'get', 'three', 'back', 'until', 'thus', 'empty', 'across', 'everywhere', 'forty', 'her', 'call', 'never', 'side', 'with', 'also', 'is', 'else', 'must', 'becomes', 'latterly', 'bill', 'few', 'those', 'same', 'co', 'de', 'cry', 'keep', 'at', 'after', 'sixty', 'whereafter', 'towards', 'between', 'your', 'cant', 'all', 'since', 'whereby', 'alone', 'many', 'what', 'nothing', 'for', 'hereafter', 'wherever', 'one', 'where', 'put', 'were', 'serious', 'fifty', 'without', 'eg', 'detail', 'which', 'of', 'any', 'un', 'rather', 'already', 'besides', 'next', 'nowhere', 'name', 'on', 'thereby', 'who', 'within', 'wherein', 'still', 'us', 'a', 'then', 'now', 'have', 'had', 'hundred', 'together', 'therein', 'anything', 'ever', 'amount', 'nor', 'may', 'very', 'see', 'over', 'was', 'how', 'con', 'no', 'about', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x1139013a0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'four', 'please', 'hundred', 'may', 'rather', 'get', 'thick', 'to', 'several', 'first', 'and', 'sixty', 'wherein', 'then', 'less', 'thereafter', 'one', 'well', 'formerly', 'hence', 'everything', 'of', 'every', 'co', 'although', 'whither', 'others', 'least', 'ever', 'with', 'whereby', 'seem', 'cry', 'her', 'his', 'ourselves', 'this', 'all', 'from', 'upon', 'even', 'often', 'among', 'therefore', 'amongst', 'three', 'most', 'done', 'nothing', 'been', 'other', 'eight', 'you', 'am', 'name', 'so', 'our', 'never', 'or', 'becomes', 'since', 'whereafter', 'herself', 'during', 'sincere', 'over', 'more', 'anyone', 'if', 'will', 'hereby', 'thin', 'interest', 'whence', 'thru', 'ten', 'nowhere', 'in', 'the', 'last', 'twelve', 'thereby', 'were', 'towards', 'these', 'whoever', 'mine', 'along', 'itself', 'within', 'that', 'now', 'un', 'themselves', 'former', 'afterwards', 'have', 'is', 'ltd', 'fifteen', 'before', 'yourselves', 'show', 'somewhere', 'up', 'yourself', 'latterly', 'across', 'why', 'eleven', 'on', 'about', 'himself', 'detail', 'amount', 'though', 'who', 'your', 'latter', 'hasnt', 'off', 'mostly', 'serious', 'can', 'down', 'beyond', 'both', 'give', 'behind', 'many', 'here', 'per', 'had', 'anyway', 'found', 'could', 'are', 'move', 'ie', 'became', 'indeed', 'same', 'elsewhere', 'keep', 'must', 'might', 'six', 'no', 'him', 'between', 'moreover', 'whatever', 'still', 'whole', 'hereupon', 'someone', 'them', 'anyhow', 'after', 'thereupon', 'wherever', 'twenty', 'would', 'there', 'else', 'whereupon', 'beside', 'fill', 'sometime', 'once', 'yours', 'inc', 'which', 'not', 'seems', 'was', 'meanwhile', 'system', 'only', 'couldnt', 'thus', 'thence', 'hereafter', 'further', 'amoungst', 'few', 'together', 'everywhere', 'because', 'nine', 'eg', 'nor', 'yet', 'bottom', 'how', 'back', 'but', 'neither', 'bill', 'he', 'it', 'already', 'through', 'five', 'via', 'find', 'under', 'de', 'whose', 'be', 'ours', 'below', 'until', 'perhaps', 'me', 'those', 'con', 'forty', 'nevertheless', 'everyone', 'its', 'describe', 'become', 'always', 'part', 'very', 'somehow', 'none', 'see', 'at', 'otherwise', 'as', 'therein', 'something', 'full', 'third', 'empty', 'into', 'fifty', 'call', 'should', 'onto', 'cant', 'front', 'noone', 'we', 'where', 'myself', 'against', 'made', 'namely', 'has', 'anywhere', 'their', 'top', 'herein', 'for', 'whereas', 'hers', 'nobody', 'mill', 'cannot', 'another', 'when', 'throughout', 'fire', 'without', 'toward', 'such', 'own', 'sometimes', 'some', 'she', 'seeming', 'besides', 'than', 'around', 'an', 'they', 'my', 'alone', 'either', 'each', 'except', 'whether', 'next', 'put', 'however', 'i', 'two', 'us', 'above', 'whom', 'beforehand', 'by', 'much', 'also', 'too', 'whenever', 'enough', 'anything', 'a', 'seemed', 'becoming', 'almost', 'out', 'etc', 'being', 'go', 'due', 'again', 'while', 'what', 'take', 'any', 're', 'do', 'side'}), vect__tokenizer=<function tokenizer_porter at 0x112e253a0>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x113024720>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'found', 'moreover', 'either', 'thereupon', 'otherwise', 'somehow', 'ours', 'if', 'but', 'anywhere', 'becoming', 'hasnt', 'you', 'his', 'ten', 'against', 'move', 'such', 'again', 'along', 'couldnt', 'hereupon', 'even', 'etc', 'nine', 'am', 'should', 'amoungst', 'give', 'into', 'made', 'onto', 'and', 'down', 'these', 'per', 'five', 'mostly', 'too', 'system', 'however', 'this', 'my', 'others', 'herein', 'elsewhere', 'much', 'whole', 'up', 'myself', 'whom', 'nobody', 'are', 'mill', 'interest', 'mine', 'indeed', 'why', 'formerly', 'almost', 'cannot', 'among', 'some', 'first', 'enough', 'thence', 'its', 'under', 'thick', 'latter', 'fifteen', 'not', 'everyone', 'though', 'by', 'beforehand', 'sometimes', 'former', 'yourselves', 'thereafter', 'beyond', 'anyway', 'something', 'an', 'except', 'themselves', 'eleven', 'hers', 'itself', 'been', 'can', 'herself', 'neither', 'ltd', 'upon', 'less', 'namely', 'each', 'thin', 'while', 'four', 'sincere', 'due', 'seems', 'here', 'perhaps', 'ie', 'he', 'none', 'most', 'to', 'both', 'top', 'in', 'could', 'own', 'third', 'amongst', 'through', 'hereby', 'off', 'toward', 'anyone', 'seem', 'take', 'yourself', 'throughout', 'further', 'meanwhile', 'our', 'therefore', 'via', 'there', 'she', 'go', 'it', 'become', 'seemed', 'so', 'several', 'will', 'sometime', 'bottom', 'someone', 'ourselves', 'fill', 'because', 'full', 'during', 'they', 'i', 'show', 'whenever', 'me', 're', 'would', 'often', 'whence', 'last', 'twelve', 'part', 'only', 'thru', 'once', 'anyhow', 'two', 'has', 'fire', 'other', 'as', 'find', 'himself', 'above', 'nevertheless', 'everything', 'them', 'six', 'we', 'be', 'somewhere', 'another', 'around', 'least', 'well', 'whose', 'whether', 'being', 'describe', 'eight', 'whereupon', 'than', 'please', 'noone', 'when', 'every', 'out', 'or', 'beside', 'seeming', 'inc', 'afterwards', 'might', 'whatever', 'always', 'from', 'although', 'their', 'twenty', 'more', 'below', 'hence', 'yet', 'before', 'the', 'whereas', 'do', 'behind', 'became', 'whither', 'front', 'that', 'yours', 'him', 'done', 'get', 'three', 'back', 'until', 'thus', 'empty', 'across', 'everywhere', 'forty', 'her', 'call', 'never', 'side', 'with', 'also', 'is', 'else', 'must', 'becomes', 'latterly', 'bill', 'few', 'those', 'same', 'co', 'de', 'cry', 'keep', 'at', 'after', 'sixty', 'whereafter', 'towards', 'between', 'your', 'cant', 'all', 'since', 'whereby', 'alone', 'many', 'what', 'nothing', 'for', 'hereafter', 'wherever', 'one', 'where', 'put', 'were', 'serious', 'fifty', 'without', 'eg', 'detail', 'which', 'of', 'any', 'un', 'rather', 'already', 'besides', 'next', 'nowhere', 'name', 'on', 'thereby', 'who', 'within', 'wherein', 'still', 'us', 'a', 'then', 'now', 'have', 'had', 'hundred', 'together', 'therein', 'anything', 'ever', 'amount', 'nor', 'may', 'very', 'see', 'over', 'was', 'how', 'con', 'no', 'about', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x11471fe20>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}), vect__tokenizer=<function tokenizer_porter at 0x106aedd00>; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1129b7c40>; total time=   2.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x110a0bc40>; total time=   2.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11085bb00>; total time=   2.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10ebbbb00>; total time=   2.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10f117c40>; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer at 0x10f263b00>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer at 0x106fc1d00>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer at 0x106fc63e0>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer at 0x10f117c40>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer at 0x10f263b00>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x110d6b740>; total time=   2.8s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer_porter at 0x1113093a0>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer_porter at 0x10f711260>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer_porter at 0x112123e20>; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}), vect__tokenizer=<function tokenizer_porter at 0x11061b100>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10b18e3e0>; total time=   2.8s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer_porter at 0x110d37b00>; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10ecaf740>; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x112ae4540>; total time=   2.9s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x113c07e20>; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x110d6b740>; total time=   2.9s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10f6c54e0>; total time=   3.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10ffefe20>; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1133c7740>; total time=   3.1s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10a91dd00>; total time=   3.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x10a9223e0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x112b7b9c0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x112ae4540>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x10a91dd00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer at 0x10a9223e0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x113ea6e80>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x112b7b9c0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x112ae4540>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x10a91dd00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x10a9223e0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x112acbc40>, vect__use_idf=False; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x113ea6e80>, vect__use_idf=False; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x112b7b9c0>, vect__use_idf=False; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10a9223e0>, vect__use_idf=False; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f1149a0>; total time=  27.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f5d09a0>; total time=  27.5s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x110e28860>; total time=  27.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f6009a0>; total time=  27.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10fab09a0>; total time=  27.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x107469d00>; total time=  28.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer at 0x10f633740>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer at 0x1109abe20>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer at 0x10f5fbb00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer at 0x107469d00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer at 0x10f633740>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer_porter at 0x10fbd53a0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer_porter at 0x1109abe20>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer_porter at 0x10f5fbb00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer_porter at 0x107469d00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'anywhere', 'detail', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}), vect__tokenizer=<function tokenizer_porter at 0x10f633740>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x112ae4540>, vect__use_idf=False; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x111028ae0>; total time=  27.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x112fb0ae0>; total time=  27.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f1a89a0>; total time=  28.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x110e789a0>; total time=  28.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x112e70720>; total time=  29.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11394c9a0>; total time=  28.3s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1109abe20>, vect__use_idf=False; total time=   5.8s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f1d0680>; total time=  28.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1113549a0>; total time=  29.2s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer at 0x110dd7c40>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f239080>; total time=  29.1s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer at 0x110d37b00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}), vect__tokenizer=<function tokenizer at 0x10ed679c0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer at 0x112123e20>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}), vect__tokenizer=<function tokenizer at 0x10ebf3b00>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer_porter at 0x111355080>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}), vect__tokenizer=<function tokenizer_porter at 0x10f239080>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10a91dd00>, vect__use_idf=False; total time=   6.2s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}), vect__tokenizer=<function tokenizer_porter at 0x110d6b740>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'above', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'whoever'}), vect__tokenizer=<function tokenizer_porter at 0x112ae4540>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}), vect__tokenizer=<function tokenizer_porter at 0x10f239080>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x1129eb880>, vect__use_idf=False; total time=   5.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x110a3f880>, vect__use_idf=False; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10ec53740>, vect__use_idf=False; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x10ed679c0>, vect__use_idf=False; total time=   4.8s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x110e879c0>, vect__use_idf=False; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x113024720>, vect__use_idf=False; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x113dfb100>, vect__use_idf=False; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x11178b100>, vect__use_idf=False; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f1149a0>, vect__use_idf=False; total time=  30.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10fab09a0>, vect__use_idf=False; total time=  30.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f5d09a0>, vect__use_idf=False; total time=  30.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x1086b5d00>, vect__use_idf=False; total time=  30.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f6009a0>, vect__use_idf=False; total time=  31.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x110e2d3a0>, vect__use_idf=False; total time=  30.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x112e70720>, vect__use_idf=False; total time=  30.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x106fc63e0>, vect__use_idf=False; total time=  29.4s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x11394c9a0>, vect__use_idf=False; total time=  30.0s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10fc1d080>, vect__use_idf=False; total time=  30.0s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f1613a0>, vect__use_idf=False; total time=  29.7s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x108baa3e0>, vect__use_idf=False; total time=  28.4s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10f1ed3a0>, vect__use_idf=False; total time=  28.8s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x10a91dd00>, vect__use_idf=False; total time=  28.2s\n",
      "[CV] END clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x112f654e0>, vect__use_idf=False; total time=  28.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:490: FitFailedWarning: \n",
      "60 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'a', 'describe', 'less', 'be', 'sixty', 'only', 'side', 'its', 'cant', 'otherwise', 'hereby', 'anyone', 'somehow', 'elsewhere', 'several', 'whenever', 'thick', 'he', 'because', 'back', 'none', 'due', 'two', 'thin', 'nevertheless', 'himself', 'neither', 'thereupon', 'so', 'made', 'becoming', 'else', 'name', 'another', 'after', 'some', 'last', 'noone', 'will', 'without', 'her', 'find', 'out', 'who', 'go', 'yourselves', 'done', 'have', 'and', 'is', 'even', 'could', 'too', 'somewhere', 'alone', 'are', 'on', 'afterwards', 'below', 'top', 'beyond', 'latter', 'anywhere', 'anyhow', 'cry', 'now', 'former', 'every', 'fifteen', 'bottom', 'nothing', 'also', 'mill', 'detail', 'into', 'part', 'seems', 'least', 'hers', 'toward', 'interest', 'me', 'amongst', 'should', 'indeed', 'five', 'ten', 'whom', 'except', 'eleven', 'meanwhile', 'fire', 'therefore', 'one', 'per', 'give', 'his', 'this', 'than', 'next', 'empty', 'whereafter', 'yourself', 'whence', 'whereby', 'there', 'hereupon', 'yours', 'wherein', 'couldnt', 'my', 'during', 'together', 'others', 'being', 'still', 'seem', 'it', 'serious', 'un', 'off', 'from', 'enough', 'eg', 'might', 'at', 'how', 'amount', 'third', 'along', 'anything', 'ours', 'may', 'de', 'take', 'by', 'has', 'front', 'can', 'was', 'both', 'etc', 'very', 'themselves', 'if', 'same', 'seeming', 'the', 'their', 'ever', 'before', 'further', 'whether', 'our', 'nor', 'everywhere', 'keep', 'move', 'inc', 'through', 'with', 'beside', 'please', 'up', 'these', 'perhaps', 'call', 'since', 'among', 'whatever', 'until', 'almost', 'thereafter', 'must', 'where', 'system', 'itself', 'what', 'all', 'mostly', 'i', 'such', 'hundred', 'but', 'hasnt', 'someone', 'they', 'sincere', 'full', 'then', 'wherever', 'most', 'besides', 'get', 'mine', 'found', 'twenty', 'which', 'above', 'moreover', 'therein', 'thereby', 'herself', 'more', 'us', 'co', 'whose', 'other', 'again', 'con', 'not', 'we', 'myself', 'to', 'upon', 'amoungst', 'via', 're', 'whither', 'however', 'that', 'twelve', 'rather', 'much', 'beforehand', 'either', 'sometimes', 'eight', 'them', 'would', 'nowhere', 'fill', 'why', 'were', 'nine', 'ie', 'you', 'am', 'fifty', 'or', 'under', 'around', 'put', 'herein', 'here', 'something', 'whereas', 'towards', 'hence', 'whole', 'see', 'ltd', 'had', 'four', 'thence', 'often', 'few', 'everything', 'onto', 'as', 'never', 'your', 'she', 'whereupon', 'own', 'him', 'any', 'hereafter', 'already', 'become', 'between', 'in', 'whoever', 'behind', 'of', 'cannot', 'anyway', 'within', 'many', 'been', 'becomes', 'against', 'forty', 'three', 'show', 'down', 'though', 'across', 'bill', 'namely', 'about', 'although', 'when', 'over', 'do', 'became', 'those', 'six', 'once', 'each', 'throughout', 'yet', 'sometime', 'first', 'formerly', 'seemed', 'an', 'for', 'everyone', 'no', 'while', 'latterly', 'nobody', 'ourselves', 'always', 'thru', 'thus', 'well'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'someone', 'within', 'over', 'also', 'under', 'further', 'were', 'nine', 'least', 'call', 'full', 'formerly', 'beyond', 'still', 'since', 'then', 'three', 'cry', 'please', 'becoming', 'how', 'against', 'around', 'whose', 'nothing', 'eight', 'here', 'why', 'below', 'six', 'all', 'seem', 're', 'ie', 'themselves', 'without', 'de', 'whole', 'always', 'myself', 'rather', 'only', 'upon', 'itself', 'becomes', 'should', 'into', 'thru', 'him', 'whereby', 'get', 'us', 'them', 'but', 'could', 'at', 'seems', 'amount', 'might', 'done', 'this', 'beside', 'bottom', 'herein', 'except', 'couldnt', 'beforehand', 'detail', 'never', 'moreover', 'more', 'seeming', 'that', 'almost', 'whoever', 'whether', 'everyone', 'hereafter', 'otherwise', 'thus', 'empty', 'indeed', 'their', 'system', 'as', 'therein', 'my', 'was', 'nobody', 'sometime', 'fifty', 'latterly', 'go', 'will', 'find', 'me', 'twelve', 'yours', 'who', 'am', 'together', 'from', 'what', 'which', 'do', 'whither', 'five', 'because', 'up', 'would', 'during', 'i', 'anywhere', 'be', 'above', 'thereafter', 'many', 'whatever', 'a', 'any', 'cannot', 'put', 'towards', 'others', 'take', 'give', 'hasnt', 'your', 'anything', 'the', 'though', 'sincere', 'thin', 'whereupon', 'third', 'made', 'former', 'meanwhile', 'it', 'have', 'so', 'back', 'ten', 'mostly', 'etc', 'inc', 'toward', 'they', 'amoungst', 'first', 'she', 'next', 'eg', 'something', 'about', 'four', 'much', 'both', 'twenty', 'afterwards', 'when', 'between', 'see', 'there', 'alone', 'yourselves', 'hence', 'fifteen', 'elsewhere', 'seemed', 'his', 'fire', 'every', 'eleven', 'if', 'some', 'by', 'perhaps', 'anyhow', 'top', 'on', 'keep', 'somewhere', 'whom', 'we', 'or', 'same', 'two', 'among', 'in', 'very', 'whenever', 'fill', 'our', 'had', 'become', 'side', 'serious', 'wherein', 'nevertheless', 'therefore', 'few', 'noone', 'move', 'now', 'neither', 'thence', 'too', 'latter', 'those', 'anyway', 'is', 'once', 'mine', 'describe', 'well', 'can', 'less', 'whence', 'of', 'co', 'himself', 'through', 'onto', 'last', 'un', 'amongst', 'may', 'has', 'thereby', 'else', 'after', 'per', 'must', 'show', 'not', 'however', 'whereafter', 'being', 'con', 'most', 'somehow', 'via', 'mill', 'herself', 'own', 'cant', 'behind', 'until', 'along', 'are', 'hundred', 'to', 'besides', 'these', 'other', 'none', 'down', 'yourself', 'again', 'before', 'name', 'out', 'nor', 'found', 'off', 'namely', 'sixty', 'her', 'due', 'its', 'hereupon', 'throughout', 'no', 'each', 'enough', 'became', 'he', 'an', 'while', 'thereupon', 'wherever', 'one', 'bill', 'although', 'part', 'thick', 'and', 'hereby', 'even', 'everything', 'sometimes', 'such', 'across', 'yet', 'already', 'ltd', 'than', 'interest', 'several', 'been', 'you', 'often', 'nowhere', 'everywhere', 'ours', 'another', 'ever', 'ourselves', 'for', 'where', 'front', 'with', 'either', 'whereas', 'anyone', 'forty', 'hers'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'down', 'about', 'or', 'seem', 'whose', 'anyhow', 'and', 'out', 'across', 'side', 'some', 'very', 'their', 'eg', 'get', 'thus', 'we', 'himself', 'back', 'after', 'amoungst', 'this', 'enough', 'please', 'no', 'as', 'top', 'up', 'anything', 'each', 'less', 'towards', 'empty', 'six', 'ourselves', 'i', 'perhaps', 'another', 'third', 'whence', 'upon', 'such', 'meanwhile', 'too', 'afterwards', 'whither', 'which', 'of', 'would', 'something', 'first', 'while', 'hasnt', 'may', 'she', 'thence', 'any', 'nine', 'though', 'until', 'must', 'on', 'what', 'not', 'amount', 'under', 'bottom', 'keep', 'several', 'more', 'nowhere', 'found', 'whereas', 'noone', 'describe', 'yet', 'these', 'fifty', 'nevertheless', 'well', 'at', 'whatever', 'give', 'from', 'even', 'system', 'although', 'everywhere', 'ten', 'ours', 'wherever', 'throughout', 'due', 'he', 'has', 'since', 'thereby', 'than', 'toward', 'else', 'elsewhere', 'ltd', 'therefore', 'but', 'same', 'ever', 'per', 'now', 'every', 'take', 'find', 'most', 'only', 'without', 'you', 'my', 'front', 'otherwise', 'all', 'myself', 'yours', 'is', 'alone', 'between', 'everything', 'me', 'thru', 'who', 'hereupon', 'part', 'see', 'it', 'becoming', 'him', 'could', 'why', 'inc', 'hence', 'became', 'whole', 'themselves', 'they', 'someone', 'for', 'always', 'done', 'forty', 'here', 'become', 'none', 'cannot', 'your', 'mostly', 'mine', 'her', 'before', 'de', 'former', 'beforehand', 'two', 'somewhere', 'with', 'herself', 'whereby', 'fill', 'ie', 'eight', 'fifteen', 'rather', 'three', 'had', 'often', 'twenty', 'are', 'bill', 'however', 'latter', 'yourself', 'whether', 'hereafter', 'by', 'one', 'might', 'am', 'where', 'off', 'around', 'least', 'much', 'whom', 'eleven', 'so', 'fire', 'nor', 'already', 'further', 'again', 'do', 'a', 'sincere', 'nobody', 'can', 'when', 'etc', 'be', 'everyone', 'seemed', 'should', 'our', 'because', 'how', 'amongst', 'there', 'also', 'wherein', 'herein', 'formerly', 'last', 'beyond', 'namely', 'both', 'behind', 'move', 'being', 'serious', 'detail', 'almost', 'few', 'via', 'during', 'own', 'therein', 'beside', 'full', 'sometime', 'us', 'along', 'latterly', 'in', 'thereupon', 'call', 'were', 'its', 'hereby', 'thick', 'either', 'against', 'others', 'to', 'cant', 'hundred', 'once', 'co', 'many', 'within', 'was', 'seems', 'over', 'among', 'anywhere', 'his', 'sixty', 'nothing', 'together', 'that', 'whereafter', 'interest', 'con', 'cry', 'whoever', 're', 'been', 'mill', 'itself', 'four', 'those', 'onto', 'becomes', 'next', 'indeed', 'thin', 'except', 'whenever', 'show', 'them', 'will', 'neither', 'then', 'an', 'un', 'if', 'hers', 'twelve', 'whereupon', 'five', 'go', 'name', 'made', 'couldnt', 'other', 'the', 'sometimes', 'thereafter', 'somehow', 'still', 'besides', 'below', 'anyone', 'anyway', 'yourselves', 'have', 'moreover', 'put', 'seeming', 'into', 'through', 'never', 'above'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'much', 'its', 'itself', 'such', 'thin', 'seeming', 'more', 'themselves', 'still', 'which', 'well', 'whence', 'please', 'herein', 'ltd', 'less', 'why', 'indeed', 'several', 'whereas', 'had', 'already', 'wherein', 'noone', 'serious', 'besides', 'nine', 'must', 'anywhere', 'ten', 'three', 'hereafter', 'do', 'everything', 'co', 'anyhow', 'how', 'hereupon', 'throughout', 'off', 'thereafter', 'un', 'somewhere', 'could', 'any', 'you', 'ever', 'whenever', 'top', 'at', 'per', 'anything', 'however', 'against', 'from', 'eg', 'his', 'even', 'my', 'find', 'someone', 'but', 'both', 'twelve', 'have', 'sometime', 'should', 'who', 'up', 'six', 'con', 'last', 'when', 'though', 'everywhere', 'found', 'enough', 'cannot', 'few', 'whereby', 'behind', 'whither', 'due', 'seems', 'for', 'next', 'something', 'wherever', 'twenty', 'done', 'own', 'namely', 'keep', 'never', 'in', 'amount', 'without', 'inc', 'call', 'me', 'being', 'into', 'not', 'thereupon', 'bill', 'of', 'sixty', 'mill', 'nevertheless', 'before', 'every', 'nobody', 'etc', 'moreover', 'same', 'be', 'yet', 'because', 'nowhere', 'empty', 'latter', 'show', 'many', 'other', 'those', 'hence', 'across', 'most', 'herself', 'all', 'our', 'has', 'amoungst', 'see', 'either', 'whereupon', 'thence', 'another', 'name', 'take', 'detail', 'ourselves', 'whatever', 'became', 'whom', 'i', 'some', 'third', 'somehow', 'by', 'an', 'none', 'were', 'very', 'seem', 'she', 'among', 'describe', 'thick', 'them', 'first', 'these', 'anyone', 'us', 'side', 'thus', 'then', 'nor', 'ours', 'further', 'sincere', 'whoever', 'if', 'around', 'the', 'himself', 'after', 'together', 'so', 'alone', 'bottom', 'him', 'former', 'your', 'back', 'beside', 'over', 'out', 'except', 'couldnt', 'above', 'almost', 'two', 'hers', 'on', 'will', 'to', 'hereby', 'whether', 'until', 'front', 'perhaps', 'part', 'de', 'becoming', 'can', 'are', 'hundred', 'their', 'beyond', 'he', 'otherwise', 'thereby', 'or', 'down', 'as', 'else', 'full', 'upon', 'meanwhile', 'they', 'onto', 'mine', 'only', 'made', 'her', 'this', 'would', 'fire', 'eight', 'within', 'here', 'myself', 'go', 'between', 'may', 'thru', 'via', 'move', 'mostly', 'and', 'formerly', 'become', 'give', 'amongst', 'whereafter', 'sometimes', 'seemed', 'ie', 'others', 'yours', 'elsewhere', 'toward', 'during', 're', 'that', 'five', 'what', 'might', 'rather', 'about', 'yourselves', 'four', 'yourself', 'now', 'fill', 'again', 'since', 'while', 'there', 'fifty', 'although', 'get', 'always', 'than', 'a', 'afterwards', 'fifteen', 'with', 'least', 'am', 'therein', 'anyway', 'cant', 'hasnt', 'eleven', 'therefore', 'we', 'forty', 'is', 'nothing', 'everyone', 'latterly', 'where', 'often', 'system', 'becomes', 'along', 'one', 'cry', 'was', 'below', 'under', 'also', 'whose', 'been', 'each', 'towards', 'it', 'whole', 'beforehand', 'interest', 'through', 'once', 'neither', 'too', 'no', 'put'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'less', 'once', 'anywhere', 'none', 'during', 'never', 'found', 'five', 'always', 'i', 'due', 'very', 'becoming', 'beforehand', 'much', 'now', 'done', 'while', 'ltd', 'along', 'eleven', 'it', 'etc', 'who', 'couldnt', 'keep', 'a', 'alone', 'hereupon', 'yourself', 'hers', 'we', 'twelve', 'hence', 'besides', 'hereafter', 'if', 'call', 'his', 'an', 'several', 'among', 'hundred', 'whereafter', 'bottom', 'six', 'those', 'he', 'therefore', 'up', 'together', 'most', 'that', 'yourselves', 'though', 'twenty', 'therein', 'whose', 'meanwhile', 'first', 'inc', 'indeed', 'into', 'except', 'detail', 'below', 'describe', 'whenever', 'themselves', 'thence', 'although', 'them', 'formerly', 'again', 'anyway', 'whither', 'wherever', 'upon', 'itself', 'herein', 'there', 'own', 'nine', 'against', 'been', 'myself', 'same', 'become', 'somehow', 'yours', 'last', 'forty', 'fifty', 'could', 'thereupon', 'give', 'above', 'least', 'at', 'system', 'be', 'eg', 'see', 'three', 'cry', 'noone', 'than', 'few', 'per', 'must', 'amongst', 'she', 'please', 'seeming', 'seemed', 'thin', 'some', 'out', 'am', 'already', 'whereas', 'thru', 'because', 'are', 'from', 'when', 'often', 'de', 'these', 'mill', 'all', 'made', 'without', 'as', 'take', 'co', 'for', 'anyhow', 'via', 'another', 'since', 'its', 'cannot', 'either', 'or', 'whence', 'their', 'why', 'fill', 'mostly', 'neither', 'by', 'put', 'sometime', 'someone', 'is', 'yet', 'somewhere', 'hereby', 'also', 'would', 'ten', 'else', 'whatever', 'still', 'go', 'to', 'the', 'not', 'something', 'which', 'sincere', 'with', 'her', 'con', 'whom', 'any', 'around', 'empty', 'nobody', 'many', 'moreover', 'in', 'interest', 'whereupon', 'nowhere', 'four', 'thereby', 'however', 'our', 'fire', 'within', 'have', 'herself', 'mine', 'latter', 'thus', 'nevertheless', 'latterly', 'whereby', 'next', 're', 'off', 'wherein', 'even', 'whoever', 'beside', 'two', 'seems', 'might', 'others', 'became', 'throughout', 'back', 'us', 'anyone', 'un', 'himself', 'everywhere', 'here', 'anything', 'ours', 'name', 'well', 'third', 'were', 'afterwards', 'every', 'until', 'how', 'they', 'part', 'ever', 'has', 'such', 'top', 'this', 'my', 'no', 'will', 'ie', 'but', 'both', 'onto', 'almost', 'nothing', 'further', 'get', 'side', 'toward', 'whole', 'thick', 'eight', 'on', 'nor', 'sometimes', 'show', 'beyond', 'only', 'over', 'through', 'whether', 'what', 'becomes', 'former', 'each', 'cant', 'perhaps', 'where', 'enough', 'namely', 'other', 'do', 'may', 'had', 'hasnt', 'me', 'full', 'otherwise', 'about', 'one', 'should', 'thereafter', 'fifteen', 'after', 'down', 'find', 'sixty', 'more', 'across', 'too', 'move', 'behind', 'so', 'everything', 'serious', 'front', 'amount', 'then', 'can', 'and', 'between', 'ourselves', 'was', 'bill', 'seem', 'you', 'being', 'him', 'of', 'rather', 'amoungst', 'everyone', 'before', 'elsewhere', 'under', 'your', 'towards'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'found', 'moreover', 'either', 'thereupon', 'otherwise', 'somehow', 'ours', 'if', 'but', 'anywhere', 'becoming', 'you', 'hasnt', 'his', 'ten', 'against', 'move', 'such', 'again', 'along', 'couldnt', 'hereupon', 'even', 'etc', 'nine', 'am', 'should', 'amoungst', 'give', 'into', 'made', 'onto', 'and', 'down', 'these', 'per', 'five', 'mostly', 'too', 'system', 'however', 'this', 'my', 'others', 'herein', 'elsewhere', 'much', 'whole', 'up', 'myself', 'whom', 'nobody', 'are', 'mill', 'interest', 'mine', 'indeed', 'why', 'formerly', 'almost', 'first', 'among', 'some', 'cannot', 'enough', 'thence', 'its', 'under', 'thick', 'latter', 'fifteen', 'not', 'everyone', 'though', 'by', 'beforehand', 'sometimes', 'former', 'yourselves', 'thereafter', 'beyond', 'anyway', 'something', 'an', 'except', 'themselves', 'eleven', 'hers', 'itself', 'been', 'can', 'herself', 'neither', 'ltd', 'upon', 'less', 'namely', 'each', 'thin', 'while', 'four', 'sincere', 'due', 'seems', 'here', 'perhaps', 'ie', 'he', 'none', 'most', 'to', 'both', 'top', 'in', 'could', 'own', 'third', 'amongst', 'through', 'hereby', 'off', 'toward', 'anyone', 'seem', 'take', 'yourself', 'throughout', 'further', 'meanwhile', 'our', 'therefore', 'via', 'there', 'she', 'go', 'it', 'become', 'seemed', 'so', 'several', 'will', 'sometime', 'bottom', 'someone', 'ourselves', 'fill', 'because', 'full', 'during', 'they', 'show', 'i', 'whenever', 'me', 're', 'would', 'often', 'whence', 'last', 'twelve', 'part', 'only', 'thru', 'once', 'anyhow', 'two', 'has', 'fire', 'other', 'as', 'find', 'himself', 'above', 'nevertheless', 'everything', 'them', 'six', 'we', 'be', 'somewhere', 'another', 'around', 'least', 'well', 'whose', 'whether', 'being', 'describe', 'eight', 'whereupon', 'than', 'please', 'noone', 'when', 'every', 'out', 'or', 'beside', 'seeming', 'inc', 'afterwards', 'might', 'whatever', 'always', 'from', 'although', 'their', 'twenty', 'more', 'below', 'hence', 'yet', 'before', 'the', 'whereas', 'do', 'behind', 'became', 'whither', 'front', 'that', 'yours', 'him', 'done', 'get', 'three', 'back', 'until', 'thus', 'empty', 'across', 'everywhere', 'forty', 'her', 'call', 'never', 'side', 'with', 'also', 'is', 'else', 'must', 'becomes', 'latterly', 'bill', 'few', 'those', 'same', 'co', 'de', 'cry', 'keep', 'at', 'after', 'sixty', 'whereafter', 'towards', 'between', 'your', 'cant', 'all', 'since', 'whereby', 'alone', 'many', 'what', 'nothing', 'for', 'hereafter', 'wherever', 'one', 'where', 'put', 'were', 'serious', 'fifty', 'without', 'eg', 'detail', 'which', 'of', 'any', 'un', 'rather', 'already', 'besides', 'next', 'nowhere', 'name', 'on', 'thereby', 'who', 'within', 'wherein', 'still', 'us', 'a', 'then', 'now', 'have', 'had', 'hundred', 'together', 'therein', 'anything', 'ever', 'amount', 'nor', 'may', 'very', 'see', 'over', 'was', 'how', 'con', 'no', 'about', 'whoever'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'four', 'please', 'hundred', 'may', 'rather', 'get', 'thick', 'to', 'several', 'first', 'and', 'sixty', 'wherein', 'then', 'less', 'thereafter', 'one', 'well', 'formerly', 'hence', 'everything', 'of', 'every', 'co', 'although', 'whither', 'others', 'least', 'ever', 'with', 'whereby', 'seem', 'cry', 'her', 'his', 'ourselves', 'this', 'all', 'from', 'upon', 'even', 'often', 'among', 'therefore', 'amongst', 'three', 'most', 'done', 'nothing', 'been', 'other', 'eight', 'you', 'am', 'name', 'so', 'our', 'never', 'or', 'becomes', 'since', 'whereafter', 'herself', 'during', 'sincere', 'over', 'more', 'anyone', 'if', 'will', 'hereby', 'thin', 'interest', 'whence', 'thru', 'ten', 'nowhere', 'in', 'the', 'last', 'twelve', 'thereby', 'were', 'towards', 'these', 'whoever', 'mine', 'along', 'itself', 'within', 'that', 'now', 'un', 'themselves', 'former', 'afterwards', 'have', 'is', 'ltd', 'fifteen', 'before', 'yourselves', 'show', 'somewhere', 'up', 'yourself', 'latterly', 'across', 'why', 'eleven', 'on', 'about', 'himself', 'detail', 'amount', 'though', 'who', 'your', 'latter', 'hasnt', 'off', 'mostly', 'serious', 'can', 'down', 'beyond', 'both', 'give', 'behind', 'many', 'here', 'per', 'had', 'anyway', 'found', 'could', 'are', 'move', 'ie', 'became', 'indeed', 'same', 'elsewhere', 'keep', 'must', 'might', 'six', 'no', 'him', 'between', 'moreover', 'whatever', 'still', 'whole', 'hereupon', 'someone', 'them', 'anyhow', 'after', 'thereupon', 'wherever', 'twenty', 'would', 'there', 'else', 'whereupon', 'beside', 'fill', 'sometime', 'once', 'yours', 'inc', 'which', 'not', 'seems', 'was', 'meanwhile', 'system', 'only', 'couldnt', 'thus', 'thence', 'hereafter', 'further', 'amoungst', 'few', 'together', 'everywhere', 'because', 'nine', 'eg', 'nor', 'yet', 'bottom', 'how', 'back', 'but', 'neither', 'bill', 'he', 'it', 'already', 'through', 'five', 'via', 'find', 'under', 'de', 'whose', 'be', 'ours', 'below', 'until', 'perhaps', 'me', 'those', 'con', 'forty', 'nevertheless', 'everyone', 'its', 'describe', 'become', 'always', 'part', 'very', 'somehow', 'none', 'see', 'at', 'otherwise', 'as', 'therein', 'something', 'full', 'third', 'empty', 'into', 'fifty', 'call', 'should', 'onto', 'cant', 'front', 'noone', 'we', 'where', 'myself', 'against', 'made', 'namely', 'has', 'anywhere', 'their', 'top', 'herein', 'for', 'whereas', 'hers', 'nobody', 'mill', 'cannot', 'another', 'when', 'throughout', 'fire', 'without', 'toward', 'such', 'own', 'sometimes', 'some', 'she', 'seeming', 'besides', 'than', 'around', 'an', 'they', 'my', 'alone', 'either', 'each', 'except', 'whether', 'next', 'put', 'however', 'i', 'two', 'us', 'above', 'whom', 'beforehand', 'by', 'much', 'also', 'too', 'whenever', 'enough', 'anything', 'a', 'seemed', 'becoming', 'almost', 'out', 'etc', 'being', 'go', 'due', 'again', 'while', 'what', 'take', 'any', 're', 'do', 'side'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'at', 'go', 'could', 'own', 'somewhere', 'nobody', 'top', 'made', 'who', 'why', 'further', 'over', 'in', 'eg', 'hereby', 'more', 'will', 'back', 'ten', 'any', 'onto', 'whereupon', 'always', 'us', 'together', 'yourself', 'wherever', 'without', 'beforehand', 'on', 'give', 'must', 'him', 'its', 'almost', 'besides', 'been', 'around', 'seem', 'eight', 'twelve', 'thereafter', 'un', 'are', 'elsewhere', 'their', 'bill', 'same', 'whither', 'name', 'wherein', 'while', 'beside', 'becoming', 'how', 'because', 'see', 'during', 'interest', 'sometimes', 'whether', 'couldnt', 'every', 'somehow', 'can', 'thence', 'enough', 'whereafter', 'full', 'otherwise', 'moreover', 'themselves', 'least', 'three', 'five', 'ours', 'still', 'etc', 'were', 'what', 'everything', 'first', 'off', 'few', 'behind', 'meanwhile', 'whole', 'now', 'whenever', 'forty', 'some', 'perhaps', 'whom', 'cant', 'much', 'which', 'afterwards', 'may', 'the', 'toward', 'not', 'across', 'me', 'nowhere', 'con', 'yourselves', 'therein', 'from', 'found', 'to', 'under', 'do', 'rather', 'if', 'hence', 'after', 'anyone', 'hundred', 'everyone', 'done', 'often', 'nine', 'next', 'cannot', 'for', 'none', 'de', 'might', 'they', 'as', 'cry', 'per', 'too', 'amount', 'have', 'thereupon', 'ltd', 'most', 'such', 'where', 'other', 'his', 'indeed', 'hers', 'mine', 'everywhere', 'down', 'those', 'empty', 'becomes', 'due', 'of', 'however', 'than', 'thereby', 'that', 'was', 'beyond', 'both', 'became', 'though', 'would', 'along', 'all', 'you', 'latterly', 'against', 'sixty', 'whoever', 'herein', 'four', 'put', 'part', 'by', 'system', 'co', 'namely', 'be', 'whereas', 'mill', 'fifty', 'himself', 'ourselves', 'hereafter', 'one', 'either', 'about', 'we', 'herself', 'anywhere', 'ever', 're', 'noone', 'anything', 'something', 'had', 'then', 'twenty', 'your', 'and', 'this', 'another', 'i', 'hereupon', 'front', 'mostly', 'upon', 'once', 'eleven', 'again', 'anyway', 'amongst', 'others', 'many', 'very', 'whence', 'myself', 'before', 'inc', 'someone', 'please', 'so', 'side', 'whereby', 'fifteen', 'within', 'itself', 'latter', 'describe', 'via', 'well', 'even', 'seeming', 'throughout', 'my', 'through', 'nothing', 'although', 'these', 'thin', 'also', 'below', 'yet', 'them', 'six', 'formerly', 'seemed', 'is', 'a', 'former', 'above', 'seems', 'nevertheless', 'whose', 'therefore', 'thick', 'else', 'it', 'yours', 'already', 'here', 'her', 'alone', 'up', 'several', 'last', 'until', 'there', 'move', 'our', 'or', 'fire', 'sincere', 'no', 'she', 'since', 'hasnt', 'only', 'less', 'take', 'he', 'out', 'keep', 'when', 'except', 'an', 'two', 'serious', 'anyhow', 'amoungst', 'am', 'between', 'thru', 'fill', 'among', 'has', 'become', 'thus', 'find', 'get', 'bottom', 'never', 'should', 'detail', 'with', 'nor', 'towards', 'but', 'call', 'ie', 'each', 'sometime', 'being', 'third', 'neither', 'show', 'whatever', 'into'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'nothing', 'describe', 'latter', 'beyond', 'please', 'however', 'will', 'a', 'all', 'whoever', 'least', 'therein', 'along', 'thereby', 'further', 'get', 'whereafter', 'herein', 'yours', 'eleven', 'some', 'else', 'through', 'hasnt', 'within', 'whither', 'eight', 'would', 'fill', 'last', 'anywhere', 'go', 'very', 'much', 'before', 'still', 'seeming', 'next', 'where', 'other', 'we', 'whereas', 'has', 'anyone', 'sixty', 'must', 'who', 'about', 'at', 'enough', 'nobody', 'itself', 'ours', 'them', 'whereby', 'serious', 'whose', 'such', 'full', 'keep', 'myself', 'even', 'thin', 'via', 'ourselves', 'but', 'anyway', 'too', 'cannot', 'twelve', 'which', 'yourselves', 'your', 'something', 'everywhere', 'their', 'above', 'less', 'might', 'alone', 'two', 'can', 'side', 'be', 'nor', 'former', 'of', 'several', 'forty', 'than', 'ten', 'her', 'me', 'whenever', 'was', 'con', 'done', 'while', 'himself', 'find', 'formerly', 'namely', 'onto', 'whether', 'etc', 'to', 'sometimes', 'yet', 'everything', 'among', 'may', 'someone', 'across', 'whatever', 'and', 'nine', 'perhaps', 'since', 'against', 'thru', 'three', 'with', 'indeed', 'on', 'de', 'these', 'when', 'though', 'seems', 'sometime', 'him', 'only', 'rather', 'those', 'had', 'noone', 'there', 'could', 'sincere', 'thus', 'mine', 'many', 'detail', 'am', 'moreover', 'until', 'top', 'give', 'up', 'somehow', 'anything', 'yourself', 'is', 'either', 'somewhere', 'hence', 'here', 'it', 'front', 'no', 'couldnt', 'the', 'from', 'off', 'co', 'because', 'that', 'made', 'why', 'this', 'amongst', 'after', 'twenty', 'none', 'each', 'by', 'becoming', 'more', 'amoungst', 'eg', 'thereafter', 'due', 'take', 'move', 'do', 'five', 'never', 'elsewhere', 'anyhow', 'ie', 'without', 'part', 'call', 'interest', 'see', 'often', 'how', 'per', 'for', 'hereby', 'what', 'neither', 'thick', 'hundred', 'became', 'whom', 'throughout', 'towards', 'thence', 'put', 'besides', 'third', 'being', 'almost', 'over', 'bottom', 'meanwhile', 'not', 'us', 'nowhere', 'seemed', 'i', 'mill', 'otherwise', 'every', 'into', 'four', 'if', 'first', 'behind', 'fifty', 'once', 'wherein', 'own', 'others', 'beside', 'fifteen', 'thereupon', 'hers', 'one', 'now', 'hereupon', 'show', 'around', 'although', 'another', 'whence', 'hereafter', 'are', 'name', 'back', 'become', 'well', 'cry', 'except', 'seem', 're', 'already', 'ever', 'whereupon', 'wherever', 'down', 'always', 'toward', 'same', 'been', 'together', 'again', 'afterwards', 'under', 'between', 'un', 'therefore', 'six', 'inc', 'then', 'herself', 'bill', 'out', 'his', 'you', 'cant', 'its', 'he', 'both', 'during', 'everyone', 'most', 'system', 'they', 'have', 'upon', 'so', 'in', 'ltd', 'beforehand', 'empty', 'or', 'my', 'themselves', 'few', 'mostly', 'were', 'latterly', 'whole', 'our', 'found', 'should', 'fire', 'she', 'also', 'becomes', 'any', 'an', 'as', 'nevertheless', 'amount', 'below'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'along', 'because', 'found', 'fire', 'who', 'four', 'she', 'their', 'each', 'same', 'always', 'my', 'everywhere', 'all', 'herself', 'one', 'thereupon', 'inc', 'whose', 'there', 'whence', 'below', 'do', 'onto', 'nevertheless', 'forty', 'that', 'least', 'anyhow', 'ours', 'amoungst', 'etc', 'but', 'between', 'towards', 'any', 'to', 'upon', 'ourselves', 'should', 'whereby', 'alone', 'whom', 'noone', 'much', 'eleven', 'detail', 'anywhere', 'now', 'though', 'seemed', 'name', 'cry', 'never', 'your', 'until', 'perhaps', 'except', 'being', 'whatever', 'again', 'others', 'since', 'in', 'thin', 'per', 'next', 'another', 'became', 'system', 'you', 'former', 'sometime', 'bottom', 'with', 'fifteen', 'it', 'although', 'as', 'across', 'otherwise', 're', 'into', 'enough', 'they', 'own', 'becoming', 'over', 'also', 'at', 'interest', 'six', 'among', 'a', 'anyone', 'couldnt', 'from', 'above', 'once', 'had', 'almost', 'seems', 'when', 'many', 'mine', 'two', 'we', 'no', 'nobody', 'therefore', 'me', 'has', 'sometimes', 'itself', 'the', 'made', 'without', 'by', 'bill', 'her', 'ie', 'please', 'here', 'around', 'wherein', 'seeming', 'beside', 'describe', 'meanwhile', 'often', 'ten', 'first', 'several', 'why', 'whither', 'sincere', 'up', 'sixty', 'throughout', 'anything', 'would', 'twelve', 'than', 'his', 'other', 'an', 'show', 'keep', 'themselves', 'was', 'under', 'are', 'therein', 'whoever', 'our', 'toward', 'while', 'down', 'how', 'mill', 'within', 'whereupon', 'already', 'out', 'very', 'third', 'him', 'yourself', 'himself', 'somehow', 'not', 'were', 'wherever', 'might', 'most', 'still', 'everyone', 'of', 'cannot', 'neither', 'nine', 'twenty', 'take', 'see', 'someone', 'call', 'due', 'latterly', 'this', 'top', 'few', 'get', 'have', 'de', 'ever', 'where', 'afterwards', 'fifty', 'formerly', 'behind', 'us', 'been', 'such', 'yourselves', 'eg', 'further', 'more', 'nothing', 'mostly', 'full', 'he', 'during', 'herein', 'via', 'before', 'nowhere', 'empty', 'namely', 'thereby', 'fill', 'three', 'so', 'thus', 'whole', 'con', 'some', 'am', 'and', 'will', 'anyway', 'both', 'these', 'those', 'is', 'beyond', 'serious', 'co', 'then', 'myself', 'even', 'its', 'else', 'nor', 'together', 'beforehand', 'part', 'last', 'front', 'none', 'or', 'if', 'whenever', 'what', 'amongst', 'find', 'could', 'go', 'must', 'be', 'move', 'put', 'them', 'thick', 'give', 'thereafter', 'five', 'eight', 'thru', 'however', 'elsewhere', 'hasnt', 'hereafter', 'less', 'seem', 'side', 'become', 'on', 'can', 'un', 'cant', 'back', 'whereafter', 'whether', 'for', 'becomes', 'only', 'hence', 'about', 'against', 'yet', 'latter', 'too', 'done', 'thence', 'may', 'yours', 'hers', 'well', 'indeed', 'hundred', 'either', 'everything', 'i', 'every', 'moreover', 'rather', 'amount', 'hereby', 'besides', 'somewhere', 'which', 'off', 'ltd', 'something', 'after', 'whereas', 'through', 'hereupon'}) instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 833, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1336, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 613, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 547, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/joblib/memory.py\", line 326, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/pipeline.py\", line 1484, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1329, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 492, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'also', 'becoming', 'hereafter', 'further', 'such', 'as', 'two', 'sometimes', 'nothing', 'your', 'every', 'out', 'whenever', 'perhaps', 'ourselves', 'upon', 'call', 'anywhere', 'hereby', 'again', 'therein', 'bottom', 'on', 'have', 'hasnt', 'latterly', 'behind', 'get', 'this', 'several', 'empty', 'well', 'anyhow', 'latter', 'become', 'fire', 'are', 'so', 'none', 'always', 'off', 'below', 'us', 'system', 'may', 'already', 'be', 'once', 'detail', 'third', 'thereafter', 'fifteen', 'per', 'wherever', 'her', 'next', 'whereafter', 'since', 'myself', 'if', 'around', 'last', 'how', 'via', 'give', 'very', 'beforehand', 'why', 'of', 'fifty', 'move', 'might', 'must', 'whether', 'at', 'first', 'that', 'here', 'over', 'most', 'twenty', 'been', 'found', 'an', 'mill', 'still', 'amongst', 'the', 'being', 'everything', 'de', 'co', 'itself', 'hundred', 'describe', 'had', 'beside', 'ever', 'and', 'could', 'above', 'yourselves', 'somehow', 'others', 'eg', 'please', 'full', 'never', 'side', 'everywhere', 'where', 'them', 'now', 'all', 'something', 'show', 'nobody', 'with', 'someone', 'whom', 'whereas', 'he', 'six', 'somewhere', 'name', 'their', 're', 'we', 'whereby', 'besides', 'alone', 'even', 'whoever', 'less', 'namely', 'than', 'to', 'through', 'it', 'any', 'inc', 'no', 'hers', 'whose', 'among', 'everyone', 'amoungst', 'more', 'although', 'many', 'afterwards', 'from', 'but', 'another', 'bill', 'i', 'without', 'seems', 'should', 'nor', 'each', 'whither', 'eleven', 'forty', 'anything', 'cry', 'twelve', 'within', 'eight', 'became', 'un', 'would', 'mostly', 'after', 'who', 'whole', 'sixty', 'do', 'thence', 'between', 'in', 'ie', 'sincere', 'because', 'noone', 'mine', 'few', 'these', 'has', 'ten', 'under', 'am', 'together', 'enough', 'con', 'its', 'find', 'four', 'by', 'up', 'sometime', 'against', 'wherein', 'some', 'ltd', 'our', 'own', 'etc', 'hereupon', 'put', 'neither', 'interest', 'five', 'though', 'back', 'throughout', 'down', 'she', 'too', 'due', 'much', 'a', 'front', 'made', 'thru', 'except', 'themselves', 'for', 'whatever', 'often', 'seeming', 'when', 'thin', 'him', 'is', 'my', 'meanwhile', 'herself', 'moreover', 'becomes', 'beyond', 'both', 'three', 'take', 'other', 'serious', 'about', 'while', 'nevertheless', 'seem', 'formerly', 'towards', 'you', 'part', 'top', 'thick', 'toward', 'yourself', 'nine', 'either', 'whence', 'they', 'during', 'amount', 'hence', 'thereupon', 'almost', 'least', 'indeed', 'however', 'same', 'which', 'then', 'what', 'his', 'anyone', 'one', 'whereupon', 'only', 'couldnt', 'yet', 'done', 'go', 'along', 'me', 'anyway', 'cannot', 'himself', 'not', 'there', 'yours', 'elsewhere', 'were', 'see', 'was', 'fill', 'ours', 'onto', 'thus', 'herein', 'into', 'across', 'nowhere', 'seemed', 'therefore', 'rather', 'until', 'before', 'otherwise', 'thereby', 'else', 'or', 'former', 'can', 'keep', 'cant', 'will', 'those'}) instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1137: UserWarning: One or more of the test scores are non-finite: [ nan  nan 0.89 0.89  nan  nan 0.9  0.89  nan  nan 0.89 0.88  nan  nan\n",
      " 0.88 0.87  nan  nan 0.88 0.87  nan  nan 0.87 0.86]\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/shivesh/Desktop/PythonProject/Sentiment Analysis/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.897\n",
      "Best params: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x1123a6d40>}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why 5-Fold Stratified Cross-Validation?\n",
    "\n",
    "We use **k-fold cross-validation** to reduce the influence of “luck” in our\n",
    "evaluation. Instead of relying on a single train/test split, we split the data\n",
    "into *k* folds and run *k* rounds of training + validation, each time using a\n",
    "different fold as the validation set. The final score is the average accuracy\n",
    "across all folds.\n",
    "\n",
    "For classification tasks we use **StratifiedKFold**, which keeps the class\n",
    "distribution (positive / negative) similar in every fold. This makes each fold\n",
    "representative of the full dataset.\n",
    "\n",
    "Choosing **5 folds instead of 10** is a practical trade-off:\n",
    "\n",
    "- 10-fold CV has slightly lower variance in the accuracy estimate,\n",
    "  but takes about **twice as long**.\n",
    "- 5-fold CV is **much faster** while still giving a reliable estimate.\n",
    "\n",
    "On the 50k IMDB reviews, a large grid search with 10-fold CV would be very\n",
    "expensive, so the book uses **5-fold stratified CV** as a good balance between\n",
    "runtime and robustness."
   ],
   "id": "fc4d4672c68581bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:43:18.787322Z",
     "start_time": "2025-12-18T16:43:17.119645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Evaluate on the test set\n",
    "\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(\"Test Accuracy: %.3f\" % clf.score(X_test, y_test))"
   ],
   "id": "b3ad42b6c7f8d3af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Out-of-Core Learning with HashingVectorizer + SGDClassifier\n",
    "\n",
    "The previous grid search uses **all 50,000 reviews in memory** at once.\n",
    "For much larger datasets, this can become too slow or memory-heavy.\n",
    "\n",
    "To handle larger data, we can use **out-of-core learning**:\n",
    "\n",
    "- Instead of loading the whole dataset, we **stream** documents from disk in\n",
    "  small mini-batches.\n",
    "- We use a classifier that supports **incremental learning** via `partial_fit`\n",
    "  (here: `SGDClassifier` with logistic loss).\n",
    "- We use `HashingVectorizer` instead of `CountVectorizer` / `TfidfVectorizer`:\n",
    "  - HashingVectorizer maps tokens to a fixed-size feature space using a hash\n",
    "    function.\n",
    "  - It does not need to store a vocabulary, so it is very memory efficient.\n",
    "  - This is ideal when we process data in a stream.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Tokenizer function**\n",
    "\n",
    "   We reuse a text cleaning + tokenization function that:\n",
    "   - removes HTML tags and punctuation,\n",
    "   - extracts emoticons,\n",
    "   - lowercases text,\n",
    "   - optionally removes stop words,\n",
    "   - optionally applies stemming.\n",
    "\n",
    "2. **Document stream**\n",
    "\n",
    "   We define `stream_docs(path)`:\n",
    "\n",
    "   - opens `movie_data.csv`,\n",
    "   - skips the header,\n",
    "   - yields one `(text, label)` pair at a time.\n",
    "\n",
    "3. **Mini-batch function**\n",
    "\n",
    "   `get_minibatch(doc_stream, size)`:\n",
    "\n",
    "   - pulls `size` documents from the stream,\n",
    "   - returns `X` (list of text) and `y` (list/array of labels).\n",
    "\n",
    "4. **Model**\n",
    "\n",
    "   - `HashingVectorizer` with our tokenizer and preprocessor.\n",
    "   - `SGDClassifier(loss='log_loss')` for online logistic regression.\n",
    "   - We call `partial_fit` on each mini-batch.\n",
    "\n",
    "We iterate, for example, over **45 mini-batches** of size 1,000:\n",
    "\n",
    "- 45 × 1,000 = 45,000 documents for training.\n",
    "- We keep the last 5,000 documents as a held-out test set.\n",
    "- At the end, we compute accuracy on that test set.\n",
    "\n",
    "The accuracy is slightly lower than the full grid-search model (~0.86–0.87 vs\n",
    "~0.90), but the training is:\n",
    "\n",
    "- much faster,\n",
    "- uses much less memory,\n",
    "- scalable to much larger datasets.\n",
    "\n",
    "This is the main idea behind **online / streaming learning** in this chapter."
   ],
   "id": "89a4a9d8532eb860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:43:25.476700Z",
     "start_time": "2025-12-18T16:43:21.996790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# If not already defined, reuse our preprocessor/tokenizer here.\n",
    "# I'll show a self-contained version which is close to the book:\n",
    "\n",
    "stop = ENGLISH_STOP_WORDS\n",
    "\n",
    "def preprocessor_stream(text):\n",
    "    # strip HTML tags\n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)\n",
    "\n",
    "    # extract emoticons\n",
    "    emoticons = re.findall(r\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "\n",
    "    # remove non-word characters and convert to lower case\n",
    "    text = re.sub(r\"[\\W]+\", \" \", text.lower())\n",
    "\n",
    "    # append emoticons without hyphens\n",
    "    text = text + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenizer_stream(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# Document stream generator\n",
    "\n",
    "def stream_docs(path):\n",
    "    \"\"\"Yield (text, label) pairs from the movie_data.csv file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # skip header\n",
    "        for line in reader:\n",
    "            text, label = line[0], int(line[1])\n",
    "            yield text, label\n",
    "\n",
    "\n",
    "# Minibatch helper\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    \"\"\"Read `size` documents from the stream.\"\"\"\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, np.array(y)\n",
    "\n",
    "\n",
    "# HashingVectorizer + SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(\n",
    "    decode_error=\"ignore\",\n",
    "    n_features=2**21,           # as in the book\n",
    "    preprocessor=preprocessor_stream,\n",
    "    tokenizer=tokenizer_stream\n",
    ")\n",
    "\n",
    "clf = SGDClassifier(\n",
    "    loss=\"log_loss\",            # logistic regression\n",
    "    random_state=1,\n",
    "    max_iter=1                  # we'll control epochs via partial_fit\n",
    ")\n",
    "\n",
    "doc_stream = stream_docs(\"movie_data.csv\")\n",
    "\n",
    "# Classes need to be passed for the first call to partial_fit\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "\n",
    "# Online training on 45 mini-batches of 1,000 docs each\n",
    "\n",
    "from pyprind import ProgBar\n",
    "pbar = ProgBar(45)\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "\n",
    "\n",
    "# Evaluate on the remaining 5,000 docs\n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(\"Accuracy: %.3f\" % clf.score(X_test, y_test))\n",
    "\n",
    "# Optionally, update the model one last time on the test set\n",
    "clf.partial_fit(X_test, y_test)"
   ],
   "id": "cd65ca7529533b2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n",
      "Accuracy: 0.830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log_loss', max_iter=1, random_state=1)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=1, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SGDClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('loss',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=loss,-%7B%27hinge%27%2C%20%27log_loss%27%2C%20%27modified_huber%27%2C%20%27squared_hinge%27%2C%20%20%20%20%20%20%20%20%27perceptron%27%2C%20%27squared_error%27%2C%20%27huber%27%2C%20%27epsilon_insensitive%27%2C%20%20%20%20%20%20%20%20%27squared_epsilon_insensitive%27%7D%2C%20default%3D%27hinge%27\">\n",
       "            loss\n",
       "            <span class=\"param-doc-description\">loss: {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'<br><br>The loss function to be used.<br><br>- 'hinge' gives a linear SVM.<br>- 'log_loss' gives logistic regression, a probabilistic classifier.<br>- 'modified_huber' is another smooth loss that brings tolerance to<br>  outliers as well as probability estimates.<br>- 'squared_hinge' is like hinge but is quadratically penalized.<br>- 'perceptron' is the linear loss used by the perceptron algorithm.<br>- The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and<br>  'squared_epsilon_insensitive' are designed for regression but can be useful<br>  in classification as well; see<br>  :class:`~sklearn.linear_model.SGDRegressor` for a description.<br><br>More details about the losses formulas can be found in the :ref:`User Guide<br><sgd_mathematical_formulation>` and you can find a visualisation of the loss<br>functions in<br>:ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;log_loss&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=penalty,-%7B%27l2%27%2C%20%27l1%27%2C%20%27elasticnet%27%2C%20None%7D%2C%20default%3D%27l2%27\">\n",
       "            penalty\n",
       "            <span class=\"param-doc-description\">penalty: {'l2', 'l1', 'elasticnet', None}, default='l2'<br><br>The penalty (aka regularization term) to be used. Defaults to 'l2'<br>which is the standard regularizer for linear SVM models. 'l1' and<br>'elasticnet' might bring sparsity to the model (feature selection)<br>not achievable with 'l2'. No penalty is added when set to `None`.<br><br>You can see a visualisation of the penalties in<br>:ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=alpha,-float%2C%20default%3D0.0001\">\n",
       "            alpha\n",
       "            <span class=\"param-doc-description\">alpha: float, default=0.0001<br><br>Constant that multiplies the regularization term. The higher the<br>value, the stronger the regularization. Also used to compute the<br>learning rate when `learning_rate` is set to 'optimal'.<br>Values must be in the range `[0.0, inf)`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=l1_ratio,-float%2C%20default%3D0.15\">\n",
       "            l1_ratio\n",
       "            <span class=\"param-doc-description\">l1_ratio: float, default=0.15<br><br>The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.<br>l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.<br>Only used if `penalty` is 'elasticnet'.<br>Values must be in the range `[0.0, 1.0]` or can be `None` if<br>`penalty` is not `elasticnet`.<br><br>.. versionchanged:: 1.7<br>    `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.15</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=fit_intercept,-bool%2C%20default%3DTrue\">\n",
       "            fit_intercept\n",
       "            <span class=\"param-doc-description\">fit_intercept: bool, default=True<br><br>Whether the intercept should be estimated or not. If False, the<br>data is assumed to be already centered.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=max_iter,-int%2C%20default%3D1000\">\n",
       "            max_iter\n",
       "            <span class=\"param-doc-description\">max_iter: int, default=1000<br><br>The maximum number of passes over the training data (aka epochs).<br>It only impacts the behavior in the ``fit`` method, and not the<br>:meth:`partial_fit` method.<br>Values must be in the range `[1, inf)`.<br><br>.. versionadded:: 0.19</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=tol,-float%20or%20None%2C%20default%3D1e-3\">\n",
       "            tol\n",
       "            <span class=\"param-doc-description\">tol: float or None, default=1e-3<br><br>The stopping criterion. If it is not None, training will stop<br>when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive<br>epochs.<br>Convergence is checked against the training loss or the<br>validation loss depending on the `early_stopping` parameter.<br>Values must be in the range `[0.0, inf)`.<br><br>.. versionadded:: 0.19</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shuffle',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=shuffle,-bool%2C%20default%3DTrue\">\n",
       "            shuffle\n",
       "            <span class=\"param-doc-description\">shuffle: bool, default=True<br><br>Whether or not the training data should be shuffled after each epoch.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=verbose,-int%2C%20default%3D0\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int, default=0<br><br>The verbosity level.<br>Values must be in the range `[0, inf)`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('epsilon',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=epsilon,-float%2C%20default%3D0.1\">\n",
       "            epsilon\n",
       "            <span class=\"param-doc-description\">epsilon: float, default=0.1<br><br>Epsilon in the epsilon-insensitive loss functions; only if `loss` is<br>'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.<br>For 'huber', determines the threshold at which it becomes less<br>important to get the prediction exactly right.<br>For epsilon-insensitive, any differences between the current prediction<br>and the correct label are ignored if they are less than this threshold.<br>Values must be in the range `[0.0, inf)`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>The number of CPUs to use to do the OVA (One Versus All, for<br>multi-class problems) computation.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=random_state,-int%2C%20RandomState%20instance%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance, default=None<br><br>Used for shuffling the data, when ``shuffle`` is set to ``True``.<br>Pass an int for reproducible output across multiple function calls.<br>See :term:`Glossary <random_state>`.<br>Integer values must be in the range `[0, 2**32 - 1]`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_rate',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=learning_rate,-str%2C%20default%3D%27optimal%27\">\n",
       "            learning_rate\n",
       "            <span class=\"param-doc-description\">learning_rate: str, default='optimal'<br><br>The learning rate schedule:<br><br>- 'constant': `eta = eta0`<br>- 'optimal': `eta = 1.0 / (alpha * (t + t0))`<br>  where `t0` is chosen by a heuristic proposed by Leon Bottou.<br>- 'invscaling': `eta = eta0 / pow(t, power_t)`<br>- 'adaptive': `eta = eta0`, as long as the training keeps decreasing.<br>  Each time n_iter_no_change consecutive epochs fail to decrease the<br>  training loss by tol or fail to increase validation score by tol if<br>  `early_stopping` is `True`, the current learning rate is divided by 5.<br>- 'pa1': passive-aggressive algorithm 1, see [1]_. Only with `loss='hinge'`.<br>  Update is `w += eta y x` with `eta = min(eta0, loss/||x||**2)`.<br>- 'pa2': passive-aggressive algorithm 2, see [1]_. Only with<br>  `loss='hinge'`.<br>  Update is `w += eta y x` with `eta = hinge_loss / (||x||**2 + 1/(2 eta0))`.<br><br>.. versionadded:: 0.20<br>    Added 'adaptive' option.<br><br>.. versionadded:: 1.8<br>   Added options 'pa1' and 'pa2'</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;optimal&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('eta0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=eta0,-float%2C%20default%3D0.01\">\n",
       "            eta0\n",
       "            <span class=\"param-doc-description\">eta0: float, default=0.01<br><br>The initial learning rate for the 'constant', 'invscaling' or<br>'adaptive' schedules. The default value is 0.01, but note that eta0 is not used<br>by the default learning rate 'optimal'.<br>Values must be in the range `(0.0, inf)`.<br><br>For PA-1 (`learning_rate=pa1`) and PA-II (`pa2`), it specifies the<br>aggressiveness parameter for the passive-agressive algorithm, see [1] where it<br>is called C:<br><br>- For PA-I it is the maximum step size.<br>- For PA-II it regularizes the step size (the smaller `eta0` the more it<br>  regularizes).<br><br>As a general rule-of-thumb for PA, `eta0` should be small when the data is<br>noisy.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.01</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('power_t',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=power_t,-float%2C%20default%3D0.5\">\n",
       "            power_t\n",
       "            <span class=\"param-doc-description\">power_t: float, default=0.5<br><br>The exponent for inverse scaling learning rate.<br>Values must be in the range `[0.0, inf)`.<br><br>.. deprecated:: 1.8<br>    Negative values for `power_t` are deprecated in version 1.8 and will raise<br>    an error in 1.10. Use values in the range [0.0, inf) instead.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('early_stopping',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=early_stopping,-bool%2C%20default%3DFalse\">\n",
       "            early_stopping\n",
       "            <span class=\"param-doc-description\">early_stopping: bool, default=False<br><br>Whether to use early stopping to terminate training when validation<br>score is not improving. If set to `True`, it will automatically set aside<br>a stratified fraction of training data as validation and terminate<br>training when validation score returned by the `score` method is not<br>improving by at least tol for n_iter_no_change consecutive epochs.<br><br>See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an<br>example of the effects of early stopping.<br><br>.. versionadded:: 0.20<br>    Added 'early_stopping' option</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('validation_fraction',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=validation_fraction,-float%2C%20default%3D0.1\">\n",
       "            validation_fraction\n",
       "            <span class=\"param-doc-description\">validation_fraction: float, default=0.1<br><br>The proportion of training data to set aside as validation set for<br>early stopping. Must be between 0 and 1.<br>Only used if `early_stopping` is True.<br>Values must be in the range `(0.0, 1.0)`.<br><br>.. versionadded:: 0.20<br>    Added 'validation_fraction' option</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_iter_no_change',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=n_iter_no_change,-int%2C%20default%3D5\">\n",
       "            n_iter_no_change\n",
       "            <span class=\"param-doc-description\">n_iter_no_change: int, default=5<br><br>Number of iterations with no improvement to wait before stopping<br>fitting.<br>Convergence is checked against the training loss or the<br>validation loss depending on the `early_stopping` parameter.<br>Integer values must be in the range `[1, max_iter)`.<br><br>.. versionadded:: 0.20<br>    Added 'n_iter_no_change' option</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=class_weight,-dict%2C%20%7Bclass_label%3A%20weight%7D%20or%20%22balanced%22%2C%20default%3DNone\">\n",
       "            class_weight\n",
       "            <span class=\"param-doc-description\">class_weight: dict, {class_label: weight} or \"balanced\", default=None<br><br>Preset for the class_weight fit parameter.<br><br>Weights associated with classes. If not given, all classes<br>are supposed to have weight one.<br><br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=warm_start,-bool%2C%20default%3DFalse\">\n",
       "            warm_start\n",
       "            <span class=\"param-doc-description\">warm_start: bool, default=False<br><br>When set to True, reuse the solution of the previous call to fit as<br>initialization, otherwise, just erase the previous solution.<br>See :term:`the Glossary <warm_start>`.<br><br>Repeatedly calling fit or partial_fit when warm_start is True can<br>result in a different solution than when calling fit a single time<br>because of the way the data is shuffled.<br>If a dynamic learning rate is used, the learning rate is adapted<br>depending on the number of samples already seen. Calling ``fit`` resets<br>this counter, while ``partial_fit`` will result in increasing the<br>existing counter.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('average',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=average,-bool%20or%20int%2C%20default%3DFalse\">\n",
       "            average\n",
       "            <span class=\"param-doc-description\">average: bool or int, default=False<br><br>When set to `True`, computes the averaged SGD weights across all<br>updates and stores the result in the ``coef_`` attribute. If set to<br>an int greater than 1, averaging will begin once the total number of<br>samples seen reaches `average`. So ``average=10`` will begin<br>averaging after seeing 10 samples.<br>Integer values must be in the range `[1, n_samples]`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes Classifier (short note)\n",
    "\n",
    "The chapter briefly mentions the **naive Bayes classifier** as another popular\n",
    "text classification model.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Very simple and fast to train.\n",
    "- Assumes features are **conditionally independent** given the class.\n",
    "- Works surprisingly well on many text tasks (e.g. spam filtering).\n",
    "- Often used as a strong baseline, especially with Bag-of-Words features.\n",
    "\n",
    "We are not implementing naive Bayes here, but the idea is:\n",
    "\n",
    "1. Estimate **P(word | class)** from the training corpus.\n",
    "2. For a new document, combine word probabilities to compute **P(class | doc)**.\n",
    "3. Choose the class with the higher posterior probability.\n",
    "\n",
    "---\n",
    "\n",
    "## word2vec (short note)\n",
    "\n",
    "The book also mentions **word2vec** as a more modern alternative to\n",
    "Bag-of-Words:\n",
    "\n",
    "- Instead of representing words as one-hot vectors, word2vec learns\n",
    "  **dense, low-dimensional embeddings**.\n",
    "- Words with similar meanings end up close together in this vector space.\n",
    "- Famous examples: **king –> man** and  **woman -> queen**.\n",
    "\n",
    "In this chapter we just note that:\n",
    "\n",
    "- word2vec (and more modern methods like GloVe, fastText, and transformers)\n",
    "  can capture **semantic relationships** that Bag-of-Words cannot.\n",
    "- Later chapters (and other resources) cover neural-network-based models\n",
    "  in more detail.\n",
    "\n",
    "For the IMDB sentiment project here, we stick to Bag-of-Words / TF-IDF +\n",
    "linear models (logistic regression / SGD)."
   ],
   "id": "a514250393293642"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Topic Modelling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "So far we have focused on **supervised learning**: predicting the sentiment\n",
    "label (positive/negative) given a review.\n",
    "\n",
    "In this section, we switch to an **unsupervised** task: **topic modelling**.\n",
    "\n",
    "### What is LDA?\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is a generative probabilistic model that\n",
    "tries to discover **hidden topics** in a document collection by looking at how\n",
    "words co-occur across documents.\n",
    "\n",
    "- Each **document** is modelled as a mixture of topics.\n",
    "- Each **topic** is a distribution over words.\n",
    "\n",
    "Given a **bag-of-words matrix** (documents × words), LDA decomposes it into:\n",
    "\n",
    "- a document-to-topic matrix (how much each topic contributes to a document),\n",
    "- a topic-to-word matrix (how strongly each word belongs to each topic).\n",
    "\n",
    "We must **choose the number of topics** in advance (here: 10). This is a\n",
    "hyperparameter and can be tuned.\n",
    "\n",
    "### LDA with scikit-learn\n",
    "\n",
    "Steps from the textbook:\n",
    "\n",
    "1. Load `movie_data.csv` into a DataFrame `df`.\n",
    "2. Use `CountVectorizer` to create a bag-of-words matrix `X`:\n",
    "   - remove very common words (`max_df=0.1` → ignore words in >10% of docs),\n",
    "   - limit vocabulary size (`max_features=5000`),\n",
    "   - use English stop words (`stop_words='english'`).\n",
    "3. Fit an `LatentDirichletAllocation` model with:\n",
    "   - `n_components=10` (topics),\n",
    "   - `learning_method='batch'` (use full dataset at once),\n",
    "   - `random_state=123` for reproducibility.\n",
    "4. Access `lda.components_`:\n",
    "   - shape `(n_topics, n_words)`,\n",
    "   - each row contains word importance for a given topic.\n",
    "5. For each topic, sort the word importances and print the **top N words**.\n",
    "\n",
    "The result is a set of interpretable topics, for example:\n",
    "\n",
    "- Topic 1: *worst minutes awful script stupid*\n",
    "- Topic 2: *family mother father children girl*\n",
    "- Topic 3: *american war dvd music tv*\n",
    "- …\n",
    "\n",
    "These topics give us a **high-level view** of what themes appear in the movie\n",
    "reviews without using any labels.\n",
    "\n",
    "Note: LDA here is **separate** from the sentiment classifier. In the next\n",
    "chapter the authors show how to embed the classifier into a web app; LDA is\n",
    "a standalone unsupervised example."
   ],
   "id": "30d828888451751b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:43:37.485625Z",
     "start_time": "2025-12-18T16:43:35.102877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# 1. Load movie_data and build bag-of-words matrix\n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "count = CountVectorizer(\n",
    "    max_df=0.1,           # ignore very frequent words (>10% docs)\n",
    "    max_features=5000,    # keep 5000 most frequent words\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X = count.fit_transform(df[\"review\"].values)\n",
    "\n"
   ],
   "id": "79809c238db2fcfc",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:44:52.930785Z",
     "start_time": "2025-12-18T16:43:38.599300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2. Fit LDA model with 10 topics\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,      # number of topics\n",
    "    random_state=123,\n",
    "    learning_method=\"batch\"\n",
    ")\n",
    "\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "# Shape of components_: (n_topics, n_features)\n",
    "print(\"lda.components_.shape:\", lda.components_.shape)\n"
   ],
   "id": "7be4c042c83f79e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (10, 5000)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:44:56.405381Z",
     "start_time": "2025-12-18T16:44:56.395523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 3. Print the top words per topic\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    top_indices = topic.argsort()[:-n_top_words - 1:-1]  # indices of top words\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    print(\" \".join(top_words))"
   ],
   "id": "74b488e6b7b669f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparing the Different Setups\n",
    "\n",
    "### 5-Fold Stratified Cross-Validation\n",
    "\n",
    "We use **k-fold cross-validation** to estimate how well our model will generalize and to choose good hyperparameters. In **5-fold stratified CV**:\n",
    "\n",
    "- The training set is split into 5 folds.\n",
    "- We train 5 models, each time holding out a different fold as validation.\n",
    "- Scores are averaged across folds.\n",
    "- *Stratified* means each fold keeps a similar positive/negative class ratio.\n",
    "\n",
    "Choosing **5 folds instead of 10** is a runtime vs stability trade-off:\n",
    "\n",
    "- 10-fold CV has slightly lower variance in the accuracy estimate, but is ~2× slower.\n",
    "- 5-fold CV is much faster and still reliable on a large dataset like 50k reviews.\n",
    "\n",
    "In this project we obtain **Test Accuracy ≈ 0.899** with 5-fold stratified CV and a\n",
    "grid-searched logistic regression model (TF–IDF features).\n",
    "\n",
    "---\n",
    "\n",
    "### Out-of-Core Learning with HashingVectorizer + SGDClassifier\n",
    "\n",
    "Out-of-core learning trains on **mini-batches** that are streamed from disk,\n",
    "instead of loading the full dataset into memory. Here we use:\n",
    "\n",
    "- `HashingVectorizer` for features (no stored vocabulary, uses the hashing trick),\n",
    "- `SGDClassifier(loss='log_loss', penalty='l2')` with `partial_fit` updates.\n",
    "\n",
    "This setup is very **memory-efficient** and fast for big data, but we do not run a\n",
    "full grid search and we accept some information loss from hashing. As a result,\n",
    "the accuracy (~0.84) is lower than the fully tuned in-memory logistic regression\n",
    "(~0.899), but the example demonstrates how to scale to larger datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### LDA vs Logistic Regression vs k-NN\n",
    "\n",
    "- **Logistic Regression** (this chapter’s main classifier):\n",
    "  Supervised, discriminative model that maps feature vectors to a probability of\n",
    "  the positive class. Used for sentiment classification.\n",
    "\n",
    "- **LDA (Latent Dirichlet Allocation)**:\n",
    "  Unsupervised probabilistic topic model. Takes a bag-of-words matrix and\n",
    "  decomposes it into:\n",
    "  - a document–topic matrix and\n",
    "  - a topic–word matrix.\n",
    "  Useful for discovering themes such as “family drama”, “horror”, etc., not for\n",
    "  direct sentiment labels.\n",
    "\n",
    "- **k-NN (k-Nearest Neighbors)**:\n",
    "  Supervised, non-parametric classifier that predicts a label by taking the\n",
    "  **majority vote** (mode) of the k closest training samples. Mentioned here as\n",
    "  another family of classifiers, but not used in this chapter."
   ],
   "id": "1b2a9f76ea706fe5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression vs LDA vs PCA vs Kernel PCA\n",
    "\n",
    "In this chapter we mainly use **logistic regression** for sentiment classification,\n",
    "but there are several related techniques that are easy to confuse:\n",
    "\n",
    "#### Logistic Regression\n",
    "- Supervised classifier.\n",
    "- Models \\(P(y=1 \\mid x)\\) using the logistic (sigmoid) function.\n",
    "- Learns a linear decision boundary in feature space.\n",
    "- Used here with TF–IDF features for IMDB review sentiment (positive vs negative).\n",
    "\n",
    "#### Linear Discriminant Analysis (LDA)\n",
    "- **This is a different LDA** from the topic model “Latent Dirichlet Allocation.”\n",
    "- Supervised dimensionality reduction + classifier.\n",
    "- Finds directions that:\n",
    "  - maximize the distance between class means, and\n",
    "  - minimize the variance within each class.\n",
    "- Uses label information directly and is designed to separate classes well.\n",
    "- Can be used as:\n",
    "  - a classifier in its own right, or\n",
    "  - a feature extractor before another classifier.\n",
    "\n",
    "#### PCA (Principal Component Analysis)\n",
    "- Unsupervised dimensionality reduction.\n",
    "- Ignores class labels and focuses on directions of **maximum variance**.\n",
    "- Often used for:\n",
    "  - compressing high-dimensional features,\n",
    "  - denoising,\n",
    "  - visualization (e.g. projecting to 2D/3D).\n",
    "\n",
    "#### Kernel PCA (KPCA)\n",
    "- Nonlinear extension of PCA.\n",
    "- Uses kernel functions (e.g. RBF kernel) to perform PCA in an implicit\n",
    "  high-dimensional feature space.\n",
    "- Captures **nonlinear** structure in the data, useful when linear PCA is not\n",
    "  expressive enough.\n",
    "\n",
    "In short:\n",
    "\n",
    "- **Logistic Regression** – supervised classifier for predicting labels.\n",
    "- **Linear Discriminant Analysis** – supervised dimensionality reduction that\n",
    "  explicitly tries to separate classes.\n",
    "- **PCA / Kernel PCA** – unsupervised dimensionality reduction methods that find\n",
    "  useful low-dimensional representations (linear for PCA, nonlinear for KPCA),\n",
    "  without using class labels."
   ],
   "id": "5ac231a37a9b0faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3838377fab31cae1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
